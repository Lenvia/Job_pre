[TOC]

# 操作系统

## 进程、线程、协程

**进程：** 进程是一个有独立功能的程序的一次运行活动，是系统**资源分配和独立运行的最小单位**。

**线程：** 线程是进程的一个执行单元，是**任务调度和系统执行的最小单位**。

**协程：** 协程是一种**用户态的轻量级线程**，协程的调度完全由用户控制。



#### 进程、线程、协程的区别【4】

线程与进程的比较：

- 进程是操作系统资源分配和独立运行的最小单位；线程是任务调度和系统执行的最小单位；
- 每个进程有独立的代码和数据空间，而线程只独享必不可少的资源，如寄存器和栈；
- 进程占据独立的内存，上下文进程间的切换开销比较大；线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据



线程和协程的区别：

- 内存开销：创建一个协程需要2kb，栈空间不够会自动扩容， 创建一个线程需要1M空间。 
- 创建和销毁：创建线程是在内核态，开销更大；协程是在运行时管理，属于用户态，开销小。
- 切换成本：线程切换需要保存各种寄存器；协程保存的寄存器比较少，它能执行更多的指令。基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。



#### 什么时候用线程，什么时候用协程【1】

1. 切换频繁的时候用协程

2. 资源分配少的情况下用协程

3. 需要开发稳定使用线程



#### 💧线程如何开始工作？

1. **线程请求**:
   - 当程序需要并行执行任务时，通常通过特定的系统调用或使用某种编程语言/库提供的API来请求创建一个新的线程。
   - 例如，C++的`std::thread`，Java的`Thread`类。
2. **线程描述符/结构体的分配**:
   - 操作系统维护了一个数据结构（例如线程控制块 TCB 在很多系统中），该结构描述了系统中的所有线程。当请求一个新线程时，系统会为新线程分配一个新的描述符或结构体。
3. **分配线程资源**:
   - 创建线程需要为其分配必要的资源，这包括栈空间、寄存器状态、指令指针等。
4. **初始化线程状态**:
   - 新线程的状态通常被设置为"就绪"，这意味着线程已经准备好运行，但还没有开始执行。
5. **调度**:
   - 一旦线程处于就绪状态，线程调度器（操作系统的一部分）可以选择该线程来执行。这取决于当前系统上其他线程的状态和优先级。
   - 调度器决定哪个线程应该在给定的时间片内运行。当新线程被选择时，操作系统会为该线程设置所需的CPU寄存器，然后线程开始执行。
6. **执行**:
   - 线程开始从指定的入口点（通常是在创建线程时指定的函数或方法）执行代码。这通常涉及加载该线程的指令指针到其入口点的地址。
   - 线程继续执行，直到完成其任务、被其他线程中断、达到其时间片的限制或遇到需要响应的系统事件。
7. **终止**:
   - 一旦线程完成其工作或者被显式终止，它将释放其资源并将其状态设置为"终止"。
   - 操作系统之后可能会回收线程相关的资源，例如其线程描述符和栈空间。



#### 共享内存的实现方式【1】

**基于映射的共享内存(Mapped Shared Memory)**

通过mmap()系统调用来映射匿名内存,将其作为共享内存使用。可以通过**在不同进程中传递同一个文件描述符**来实现。



**基于物理内存实现**（带名字的共享内存）

- shm_get()

  使用shmget()系统调用来创建一段共享内存，并返回一个共享内存标识符。

- shm_at(id,addr,flag)

  使用该系统调用将共享内存挂载到当前进程的地址空间

- shm_dt()

  可以删除指定的共享内存，从当前的进程的地址空间当中删除。通过shmat()返回的挂载的地址，将加到进程地址空间当中的内存删除。

- shm_ctl()

  对指定的共向内存空间进行操作。也是通过shm_get()返回的标识符来进行相关的操作。



**基于文件映射（mmap实现）**

使用OPEN系统调用以读写模式打开一个文件,这个文件起到共享内存的介质作用。

使用mmap系统调用,把这个文件映射到调用进程的地址空间,指定映射的起始地址和长度。

多个进程重复上述步骤,映射同一个文件到各自进程的地址空间。因此文件起到多个进程间共享内存的作用。

进程对映射区的读写实际就是对文件的读写,从而实现了进程间的通信。

使用munmap取消映射,关闭文件。





### 进程

####  进程状态【2】

在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。

- 运行状态（*Running*）：该时刻进程占用 CPU；
- 就绪状态（*Ready*）：可运行，由于其他进程处于运行状态而暂时停止运行；
- 阻塞状态（*Blocked*）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；

进程还有另外两个基本状态：

- 创建状态（*new*）：进程正在被创建时的状态；
- 结束状态（*Exit*）：进程正在从系统中消失时的状态；



在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。就需要一个新的状态，来**描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态**。

挂起状态可以分为两种：

- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；





#### 进程间通信【3】

每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。

**管道、消息队列、共享内存、信号量、信号、套接字**

- 管道

  管道传输数据是单向的。A进程将数据以字节流写入管道，B进程需要等待A进程将信息写完以后才能读出来。

  比如 ps -f | grep xxx

  缺点：效率低，不适合进程间频繁地交换数据

- 消息队列

  在发送数据时，按照一个个独立单元发送，发送方和接收方约定好消息的类型和格式

  缺点：不适合比较大数据的传输；**存在用户态与内核态之间的数据拷贝开销**

- 共享内存

  申请一块虚拟地址空间，不同进程通过这块虚拟地址空间映射到相同的物理地址空间。无需拷贝。

  缺点：会出现冲突

- 信号量

  防止多进程竞争共享资源，而造成的数据错乱的一个约束和保护机制。

  **信号量表示资源的数量**，控制信号量的方式有两种<font color=green>原子操作</font>：P 操作和 V 操作。P 操作为申请资源，V 操作是归还资源。

  信号初始化为 1，就代表着是**互斥信号量**，它可以保证共享内存在任何时刻只有一个进程在访问；

- 信号

  信号是进程间通信机制中唯一的**异步通信机制**，因为可以在任何时候发送信号给某一进程，一旦有信号产生，用户进程可以有几种处理方式：

  - 执行默认操作。Linux 对每种信号都规定了默认操作，例如，SIGTERM 信号，就是终止进程的意思。
  - 捕捉信号。可以为信号定义一个信号处理函数。当信号发生时，执行相应的信号处理函数。
  - 忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，它们用于在任何时候中断或结束某一进程

- 套接字

  跨网络与不同主机上的进程之间通信

  domain 参数用来指定协议族、type 参数用来指定通信特性、protocal 参数原本是用来指定通信协议的（废弃）

  - TCP 协议通信的 socket 编程模型

    服务端和客户端初始化 socket -> 服务端 bind 绑定 IP 和端口 -> 服务端调用 listen 监听 -> 服务端调用 accept 等待客户端连接 -> 客户端调用 connect 发起连接请求 -> **服务端 accept 返回用于传输 socket 的文件描述符** -> 客户端 write 服务端read 

  - UDP 协议通信的 socket 编程模型

    只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind

  - 本地进程间通信的 socket 编程模型

    本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是绑定一个本地文件，这也就是它们之间的最大区别



**信号量如何实现的？【1】**

1. 二元信号量（binary semaphore）：又称为互斥锁（mutex），只有两种状态，0和1。在任何时刻只有一个线程/进程可以获得该信号量，即只有一个线程/进程可以进入临界区。
2. 计数信号量（counting semaphore）：可以取任意整数值，用于控制对一组资源的访问。当信号量的值为正整数时，允许访问资源；当信号量的值为0时，不允许访问资源，请求资源的线程/进程会被阻塞；当信号量的值为负整数时，表示有多少个线程/进程正在等待该资源。



在实现信号量时，并发访问的情况，通常使用**原子操作、互斥锁**等技术来确保信号量的正确性。

AQS（AbstractQueuedSynchronizer）队列同步器。

1. 非公平模式

   在许可数量允许的情况下，让所有线程都进行自旋操作，而不管它们先来后到的顺序，全部线程放到一起去竞争许可。

   CAS算法保证并发修改许可值，当剩余许可数小于0时从而导致线程会进入等待队列中。

2. 公平模式

   公平模式则通过队列来实现公平机制。它会检查线程是否已经存在等待队列，如果已经有等待队列则返回-1，否则表示让AQS同步器将当前线程进入等待队列中，队列则意味着公平。





#### 进程调度算法【1】

- 先来先服务调度算法

  每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。

- 最短作业优先调度算法

  ```
  优先把短作业执行完，再执行长作业。缺点是如果短作业很多，长作业会被搁置。
  ```

  优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。

- 高响应比优先调度算法

  ```
  是在短作业优先的基础上改进，加上一个随时间叠加的权重。等待时间越长，权重越高。
  这种算法既可以优先完成短作业，又能确保长作业不会长期饥饿。
  ```

  每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行

  $$
  相应比优先级 = \frac{等待时间 + 要求服务时间}{要求服务时间}
  $$
  （高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的）

- 时间片轮转调度算法

  ```
  定义一个时间片长度，平均给每个进程分配时间片，一旦时间片用完，作业就会由进行转为就绪，等待重新被调度。缺点是如果作业比较多，那长作业可能需要好几轮才可以被执行完。
  ```

  每个进程被分配一个时间段，称为时间片（Quantum），即允许该进程在该时间段中运行。

  - 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；
  - 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

- 最高优先级调度算法

  从就绪队列中选择最高优先级的进程进行运行。

  进程的优先级可以分为，静态优先级和动态优先级：

  - 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
  - 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是随着时间的推移增加等待进程的优先级。

- 多级反馈队列调度算法

  ```
  设置了不同的队列，可以分类为高、中、低优先级。优先级越高，分配的时间片越小。首先刚来的会进入高优先级，如果没执行完进入下一级的优先级。只有上一个队列执行完后，才可以开始下一个队列。缺点还是长作业的问题，加入上一级队列一直有作业，那下一级别的队列的进程就会饥饿
  ```

  多级反馈队列（Multilevel Feedback Queue）调度算法是「时间片轮转算法」和「最高优先级算法」的综合和发展。

  <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-02-09%2016.24.52.png" alt="截屏2023-02-09 16.24.52" style="zoom: 33%;" />

  - 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
  - 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
  - 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；






### 线程

线程的优点：

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发执行；
- 各个线程之间可以共享地址空间和文件等资源；



主要有三种线程的实现方式：

- **用户线程（User Thread）**：在用户空间实现的线程，是由用户态的线程库来完成线程的管理；
- **内核线程（Kernel Thread）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（LightWeight Process）**：在内核中来支持用户线程；





#### 为什么要有多线程？【1】

- 提高CPU利用率。当一个线程发生I/O时，会把该线程从CPU上调度下来，并把其他的线程调度上去。
- 提高I/O吞吐，实现并发。通过连接池和多线程，实现每个线程使用一个连接。



### 💧从代码文本到可执行文件，再到RPC调用，这个过程中发生了什么？

【重点在编译方面】

1. **编译 Go 代码**:
   - 使用 Go 的编译器 `go build` 来编译代码。
   - Go 语言的编译器将源代码转换为机器代码，生成一个可执行文件。
2. **运行可执行文件**:
   - 当可执行文件被运行时，操作系统将其加载到内存中，并执行机器代码。
   - 如果您的 Go 代码包含一个服务器，例如使用 `net/http` 包编写的 HTTP 服务器，那么它现在会开始监听指定的端口。
3. **RPC 调用**:
   - RPC (Remote Procedure Call) 是一种通过网络在远程计算机程序之间请求服务的协议。
   - 如果你的服务使用的是 Go 的 `net/rpc` 包或者其他像 gRPC 这样的库，那么它会处理远程调用，并按照约定的格式解析这些调用。
   - 当 RPC 请求到达服务器时，服务器会解析请求，找到对应的方法或函数进行调用，并返回结果给调用方。



详细：

**编译过程**

1. **源代码分析**:
   - **词法分析**: Go 编译器首先通过词法分析器将源代码转换成一个个的令牌（tokens）。例如，`func`, `int`, 变量名和括号等都被视为单独的令牌。
   - **语法分析**: 之后，这些令牌被送入语法分析器，它会将它们组成一个**抽象语法树（AST）**。
2. **类型检查**:
   - 在这一步，编译器会检查源代码中所有的类型声明、变量、函数调用等是否都满足 Go 语言的类型系统的要求。
   - 这也是 Go 提前捕获很多错误的地方，比如不匹配的类型赋值或者函数调用。
3. **中间代码生成**:
   - 从 AST 开始，编译器开始生成中间代码，通常是静态单赋值(SSA)形式的。这是一个低级别但与特定架构无关的代码表示。
   - 在这一步，**编译器会进行很多优化**，如死代码消除、常量传播和循环展开等。
4. **机器代码生成**:
   - Go 编译器将 SSA 中间代码转换为特定于目标架构的机器代码。例如，如果你正在为 AMD64 架构编译，你会得到 AMD64 机器码。
   - 该步骤还涉及寄存器分配和其他与特定硬件架构相关的优化。
5. **链接**:
   - 机器代码生成后，编译器会进入链接阶段。链接器将所有的代码和依赖合并到一个单一的可执行文件中。
   - 如果有外部库或其他包被引用，它们也会在此阶段被加入。
   - 最后，链接器会添加启动代码，这是在程序启动时首先运行的代码段。
6. **生成可执行文件**:
   - 以上所有步骤完成后，你会得到一个包含机器码的可执行文件。这个文件可以被操作系统加载和执行。





## 💧内存

相关内容见 https://www.xiaolincoding.com/os/3_memory/vmem.html#%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98



为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套**虚拟地址空间**，每个程序只关心自己的虚拟地址就可以，分布到物理地址内存是不一样的。

每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。



那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分页**的方式，同时两者结合都是可以的。

内存分段是根据程序的逻辑角度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致外部内存碎片和内存交换效率低的问题。

于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 `4KB`。由于分了页后，就不会产生细小的内存碎片，解决了内存分段的外部内存碎片问题。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。

再来，为了解决简单分页产生的页表过大的问题，就有了**多级页表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的**局部性原理**，在 CPU 芯片中加入了 **TLB**，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。

**Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理**。于是 Linux 就把所有段的基地址设为 `0`，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。

另外，Linux 系统中虚拟空间分布可分为**用户态**和**内核态**两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。



#### 虚拟内存的实现方式【1】

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存。

操作系统通过**内存分段和内存分页**管理虚拟地址与物理地址之间的关系。



#### 虚拟内存的作用

1. 虚拟内存可以使得**进程对运行内存超过物理内存大小**，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
2. 由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就**解决了多进程之间地址冲突的问题**。
3. 页表里的页表项中除了物理地址之外，还有**一些标记属性的比特**，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。

 



## 调度

### ⭐️操作系统的调度算法【3】

- **进程调度**（详见上文进程调度算法）

  - 先来先服务调度算法
  - 最短作业优先调度算法
  - 高响应比优先调度算法
  - 时间片轮转调度算法
  - 最高优先级调度算法
  - 多级反馈队列调度算法

- **内存页面置换算法**

  - <!--最佳页面置换算法（OPT）-->

    <!--置换在「未来」最长时间不访问的页面。（很理想，但是实际系统中无法实现，因为程序访问页面时是动态的。最佳页面置换算法作用是为了衡量你的算法的效率）-->

  - 先进先出置换算法（FIFO）

    选择在内存驻留时间很长的页面进行中置换。

  - 最近最久未使用（LRU）的置换算法

    选择最长时间没有被访问的页面进行置换。

    虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

  - 时钟页面置换算法（Lock）

    把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

    当发生缺页中断时，算法首先检查表针指向的页面：

    - 如果它的访问位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
    - 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

  - 最不常用（LFU）算法

    当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰。

- **磁盘调度算法**

  - 先来先服务

  - 最短寻道时间优先（SSF）

    优先选择从当前磁头位置所需寻道时间最短的请求。

    但这个算法可能存在某些请求的饥饿，产生饥饿的原因是磁头在一小块区域来回移动。

  - 扫描算法（电梯算法）

    磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向。

    中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。

  - 循环扫描算法

    只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应一个方向上的请求。

  - LOOK 与 C-LOOK 算法

    磁头在移动到「最远的请求」位置，然后立即反向移动。





### LRU？如何实现LRU【2】

最近最久未使用（LRU）的置换算法的基本思路是，**发生缺页时，选择最长时间没有被访问的页面进行置换，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/LRU%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="最近最久未使用的置换算法" style="zoom: 33%;" />





### 操作系统的中断【1】

**中断是系统用来响应硬件设备请求的一种机制**，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。

中断处理程序在响应中断时，可能还会**「临时关闭中断」**，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。

为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」。

- **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
- **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。



### 操作系统的并发了解吗【1】

并发是指两个或多个事件可以在**同一个时间间隔**发生。





## 锁

https://www.xiaolincoding.com/os/4_process/pessim_and_optimi_lock.html#%E4%BA%92%E6%96%A5%E9%94%81%E4%B8%8E%E8%87%AA%E6%97%8B%E9%94%81

### 死锁怎么产生的？怎么避免？【1】

死锁：两个或两个以上线程都在等待对方执行完毕**释放锁**，在没有外力的作用下，这些线程会一直相互等待，就没办法继续运行，这种情况就是发生了**死锁**。

四个必要条件：

1. 互斥条件：一个资源只能被一个进程所使用
2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放
3. 不可抢占条件：进程已获得的资源，在未使用完之前，不能强行剥夺
4. 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源的关系





**如何解决？**

最常见：**使用资源有序分配法，来破循环与等待条件**。

（互斥不可破坏）

（不可抢占条件破坏代价大，实现复杂）

- 破坏请求与保持条件：

  允许进程获取初期所有资源后，便开始运行，运行过程中再逐步释放自己占有的资源

- 破坏循环与等待条件：

  对各进程**请求资源的顺序**做一个规定，避免相互等待。线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。

（注意不是执行的顺序）



### 操作系统层面上，锁是如何实现的？【1】

https://blog.csdn.net/shimadear/article/details/82823284

https://blog.csdn.net/weixin_43519984/article/details/105093934

1. 以中断启用和禁止来实现锁

   采用禁止中断，不让进程自动让出CPU，就可以**防止进程切换**，将一组操作变为原子操作。

   **在加锁操作完成后再开中断。**

2. 锁内存总线

   硬件提供了**锁内存总线**的机制，在锁内存总线的状态下执行test and set操作，就能保证同时只有一个核来test and set

   **让加锁成为原子操作。**
   
   ```
   在多 cpu 下，当其中一个处理器要对共享内存进行操作的时候，在总线上发出一个 LOCK# 信号，这个信号使得其他处理器无法通过总线来访问到共享内存中的数据，总线锁定把 CPU 和内存之间的通信锁住了
   ```
   
   





## I/O

**零拷贝和异步I/O**

在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。

传输文件的时候，我们要根据文件的大小来使用不同的方式：

- 传输大文件的时候，使用「异步 I/O + 直接 I/O」；
- 传输小文件的时候，则使用「零拷贝技术」；



传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。

为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次**系统调用**（`sendfile` 方法）合并了<font color=green>磁盘读取</font>与<font color=green>网络发送</font>两个操作，降低了上下文切换次数。另外，**拷贝数据都是发生在内核中的**，天然就降低了数据拷贝的次数。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683834797573693668646a333137323070713736712e6a7067.jpeg" alt="截屏2022-11-14 21.15.24" style="zoom:33%;" />

<center>零拷贝</center>

当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。

<img src="https://camo.githubusercontent.com/f14b6a608288d2150d8be50c932793356fd4a8524ed98bc67af72ed1bfbf5561/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683834797631787836626a333162673071343075742e6a7067" alt="截屏2022-11-14 21.15.44" style="zoom:33%;" />

<center>异步IO</center>



### ⭐️I/O 多路复用

https://zhuanlan.zhihu.com/p/126278747

由一个线程监控多个网络请求。当有数据准备就绪之后再分配对应的线程去读取数据，这么做就可以节省出大量的线程资源出来。

select，poll，epoll都是IO多路复用机制，即可以监视多个描述符，一旦某个描述符就绪（读或写就绪），能够通知程序进行相应读写操作。

**多个请求复用了一个进程，这就是多路复用**，这种思想很类似一个 CPU 并发多个进程，所以也叫做**时分多路复用**。







#### 介绍下 IO 多路复用【3】

最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。

传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，后续的读写都在对应的进程/线程。但是当随着客户端数量的增长，大量的进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。

为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的 I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。



我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，进程可以通过一个系统调用函数从内核中获取多个事件。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838347a626d6a6f67326a33313436306e796d787a2e6a7067.jpeg" alt="截屏2022-11-14 21.31.36" style="zoom:33%;" />



(fd：文件描述符)

**select**

1. 在调用select之前告诉应用进程需要监控哪些fd <u>可读、可写、异常</u> 事件，这些分别都存在一个fd_set数组中。
2. 应用进程调用 select 的时候 把3个fd_set传给内核（产生了一次fd_set在用户空间到内核空间的复制），内核收到后对fd_set进行遍历，然后一个个去扫描对应fd是否满足可读写事件。
3. 如果发现了有对应的fd有读写事件后，内核会把fd_set里 <u>没有事件状态的fd</u>  句柄清除，然后把**有事件的fd返回给应用进程**（这里又会把fd_set从内核空间复制用户空间）。
4. 最后应用进程收到了select返回的活跃事件类型的fd句柄后，再向对应的fd发起数据读取或者写入数据操作。



缺陷：

1. 每次调用select，都需要把被监控的fds集合从用户态空间拷贝到内核态空间，高并发场景下这样的拷贝会使得消耗的资源是很大的。
2. 能监听端口的数量有限，单个进程所能打开的最大连接数有FD_SETSIZE宏定义
3. 被监控的fds集合中，只要有一个有数据可读，**整个socket集合就会被遍历一次收集可读事件**：仅仅关心是否有数据可读这样一个事件



**poll**

poll模型里面通过使用链表的形式来保存自己监控的fd信息，正是这样poll模型里面是没有了连接限制。

poll使用pollfd结构而非的fd_set结构。

管理多个描述符也是进行轮询。

```
struct pollfd{
	int fd;  // 文件描述符
	short events; // 注册的事件
	short revents;  // 实际发生的事件，由内核填充
}
```



select 和 poll 简述：

```
首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。
```



**epoll**

<font color=grey>epoll_create 创建一个 epoll对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。</font>

1. 创建内核事件表（epoll_create）。

   向内核申请创建一个fd作为内核事件表（红黑树结构的文件，没有数量限制），这个fd用来保存应用进程需要监控哪些fd和对应类型的事件。

2. 添加或移出监控的fd和事件类型（epoll_ctl）

   调用此方法可以是向内核的内核事件表 动态的添加和移出fd 和对应事件类型

3. epoll_wait 绑定回调事件

   内核向事件表的fd绑定一个回调函数。

   当监控的fd活跃时，会调用回调函数把事件加到一个**活跃事件队列**里;

4. 最后在epoll_wait 返回的时候内核会把活跃事件队列里的fd和事件类型返回给应用进程。

特点：

从epoll整体思路上来看，采用**事先就在内核创建一个事件监听表，后面只需要往里面添加移出对应事件**，因为本身事件表就在内核空间，所以就避免了向select、poll一样每次都要把自己需要监听的事件列表传输过去，然后又传回来，这也就避免了事件信息需要在用户空间和内核空间相互拷贝的问题。

epoll并不是像select一样去遍历事件列表，然后逐个轮询的监控fd的事件状态，而是**事先就建立了fd与之对应的回调函数，当事件激活后主动回调callback函数**，这也就避免了遍历事件列表的这个操作，所以epoll并不会像select和poll一样随着监控的fd变多而效率降低。



优势：

1. epoll 在内核里使用红黑树来跟踪进程所有待检测的fd，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，增删改一般时间复杂度是 `O(logn)`。select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。
2. epoll 使用事件驱动的机制，内核里维护了一个链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838353177776165746c6a333078623066647766722e6a7067.jpeg" alt="截屏2022-11-14 23.01.15" style="zoom: 50%;" />

epoll 支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。

- 边缘触发模式，当被监控的 Socket 描述符上有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次
- 水平触发模式，当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，目的是告诉我们有数据需要读取；





**比较表格**

|              | select                                                       | poll                                                         | epoll                                             |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------- |
| 性能         | 随着连接数的增加，性能急剧下降，处理成千上万的并发连接数时，性能很差 | 随着连接数的增加，性能急剧下降，处理成千上万的并发连接数时，性能很差 | 随着连接数的增加，性能基本没有变化                |
| 连接数       | 一般1024                                                     | 无限制                                                       | 无限制                                            |
| 内存拷贝     | 每次调用select拷贝                                           | 每次调用poll拷贝                                             | fd首次调用epoll_ctl拷贝，每次调用epoll_wait不拷贝 |
| 数据结构     |                                                              |                                                              | 红黑树                                            |
| 内在处理机制 | 线性轮询                                                     | 线性轮询                                                     | FD挂在红黑树，通过事件回调callback                |
| 时间复杂度   | O(n)                                                         | O(n)                                                         | O(1)                                              |





#### select poll 和 epoll 最大的差别是什么【1】

select 和 poll 每次都需要两次复制、两次遍历。

epoll 仅在第一次创建红黑树，后续都是对红黑树进行管理，增删改时间复杂度 O(logn)，减少了拷贝和内存分配；事件驱动，维护活跃事件队列，只将有事件发生的 Socket 集合传递给应用程序。



#### 操作系统的 IO 模型(阻塞，非阻塞，IO多路复用，信号驱动IO，异步IO)【1】

https://blog.csdn.net/Bb15070047748/article/details/124699009

1. 阻塞IO就是当应用程序发起读取数据申请时，在内核数据没有准备好之前，应用程序会一直处于等待数据状态
2. 非阻塞IO就是当应用程序发起读取数据申请时，如果内核数据没有准备好会即刻告诉应用程序（返回错误码等），不会让程序在这里等待。一旦内核中的数据准备好了，并且又再次收到了程序的请求，那么它马上就将数据拷贝到了用户线程
3. 多路复用...
4. 信号驱动。当进程发起一个IO操作，系统调用sig action执行一个信号处理函数，该函数**向内核注册一个回调函数**，然后进程返回，并且不阻塞当前进程；当内核数据准备好时，**内核使用信号（SIGIO）通知应用线程**调用recvfrom来读取数据（运行回调函数）。
5. 在异步IO模型中，应用只需要向内核发送一个请求，告诉内核它要读取数据后即刻返回；内核收到请求后会建立一个信号联系，当数据准备就绪，**内核会主动把数据从内核复制到用户空间**（而信号驱动是告诉应用程序何时可以开始拷贝数据），异步IO模型真正的做到了完完全全的非阻塞；





#### 一次普通 IO 的过程，什么时候用到了系统调用【2】

一次普通 IO 的过程，大致分为以下几个步骤：

1. 应用程序调用高级 IO 接口，比如 fopen、read、write 等，将数据读取到缓冲区或者写入到文件中。
2. 操作系统内核将应用程序提供的参数进行解析，然后**将读写请求加入到文件描述符对应的 I/O 队列**中。
3. 内核通过调用底层驱动程序将请求传递给对应的设备驱动程序。
4. 驱动程序根据硬件特性，调用硬件进行读写操作，并**将结果写回内核缓冲区**。
5. 内核将数据**从内核缓冲区拷贝到用户缓冲区**，并返回操作结果给应用程序。

在这个过程中，操作系统内核在第2、4、5步中使用了系统调用：

- 第2步中，将请求加入到 I/O 队列时需要使用系统调用，如 Linux 中的 epoll_wait、select、poll 等等。
- 第4步中，驱动程序调用硬件进行读写操作时需要使用系统调用，如 Linux 中的 read、write 等等。
- 第5步中，内核将数据从内核缓冲区拷贝到用户缓冲区时需要使用系统调用，如 Linux 中的 read、write 等等。

因此，系统调用在普通 IO 过程中的作用非常重要，负责将应用程序与硬件设备进行连接，并提供底层的 I/O 功能。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838656575766f38317a6a333137303074613431302e6a7067.jpeg" alt="截屏2022-11-23 01.19.07" style="zoom:33%;" />





## Linux

**查看socket**

可以使用 `netstat` 或者 `ss`，这两个命令查看 socket、网络协议栈、网口以及路由表的信息。

![截屏2022-11-14 23.28.18](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683835327030326c67656a33306b393037727133712e6a7067.jpeg)

socket 的状态（*State*）、接收队列（*Recv-Q*）、发送队列（*Send-Q*）、本地地址（*Local Address*）、远端地址（*Foreign Address*）、进程 PID 和进程名称（*PID/Program name*）等。

当 socket 状态处于 `Established`时：

- *Recv-Q* 表示 socket 缓冲区中还没有被应用程序读取的字节数；
- *Send-Q* 表示 socket 缓冲区中还没有被远端主机确认的字节数；

而当 socket 状态处于 `Listen` 时：

- *Recv-Q* 表示全连接队列的长度；
- *Send-Q* 表示全连接队列的最大长度

在 TCP 三次握手过程中，当服务器收到客户端的 SYN 包后，内核会把该连接存储到半连接队列，然后再向客户端发送 SYN+ACK 包，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后<font color=red>创建新的完全的连接，并将其增加到全连接队列</font> ，等待进程调用 `accept()` 函数时把连接取出来。



**查看协议栈**

TCP显示了主动连接（*active connections openings*）、被动连接（*passive connection openings*）、失败重试（*failed connection attempts*）、发送（*segments send out*）和接收（*segments received*）的分段数量等各种信息

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383532747a6a3663706a33306968306e79676f6c2e6a7067.jpeg" alt="截屏2022-11-14 23.33.05" style="zoom:50%;" />





#### Linux如何查看哪些端口建立tcp连接？这些端口都有哪些状态？【1】

`netstat` 或者 `ss`

(listien,established,time-wait等等)。

状态：ESTABLISHED, CLOSE_WAIT, SYN_SENT, FIN_WAIT, TIME_WAIT 等等



为什么要有time-wait 见 `TCP/四次挥手`。（防止历史数据遗留；保证接收方正常关闭）



#### 敲kill -9命令发生了什么。-9是什么意思【1】

- kill -9 1050 ，表示给 PID 为 1050 的进程发送 `SIGKILL` 信号，用来立即结束该进程；



## 待分类

#### 用户态和内核态【2】

https://blog.csdn.net/m0_47221702/article/details/119947155

https://blog.csdn.net/m0_37199770/article/details/113482312

内核态：

处于内核态的 CPU 可以访问任意的数据，包括外部设备，比如网卡、硬盘等，处于内核态的 CPU 可以从一个程序切换到另外一个程序，并且占用 CPU 不会发生抢占情况。

执行内核空间的代码，有对硬件的所有操作权限，可以执行所有`CPU 指令集`，访问任意地址的内存，在内核模式下的任何异常都是灾难性的，将会导致整台机器停机



用户态：

处于用户态的 CPU 只能受限的访问内存，并且不允许访问外部设备，用户态下的 CPU 不允许独占，也就是说 CPU 能够被其他程序获取。

在用户模式下，代码没有对硬件的直接控制权限，也不能直接访问地址的内存，程序是**通过调用系统接口**来达到访问硬件和内存，在这种保护模式下，即时程序发生崩溃也是可以恢复的，在电脑上大部分程序都是在用户模式下运行的



**用户态与内核态的切换**

一次 用户态 -> 内核态 -> 用户态 的切换如下：

- 保留用户态现场（上下文、寄存器、用户栈等）
- 复制用户态参数，用户栈切到内核栈，进入内核态
- 额外的检查（因为内核代码对用户不信任）
- 执行内核态代码
- 复制内核态代码执行结果，回到用户态
- 恢复用户态现场（上下文、寄存器、用户栈等）





#### 操作系统的临界区了解吗？【1】

临界资源是一次仅允许一个进程使用的共享资源。各进程采取互斥的方式，实现共享的资源称作临界资源。

属于临界资源的硬件有：打印机，磁带机等；软件有：消息队列，变量，数组，缓冲区等。诸进程间采取互斥方式，实现对这种资源的共享。

**每个进程中访问临界资源的部分称为临界区（criticalsection），每次只允许一个进程进入临界区，进入后，不允许其他进程进入。**





#### ⚠️大端和小端有什么区别？





#### read 是用户发起还是内核发起？

在Linux系统中，read操作既可以是用户进程发起的，也可以是内核发起的。

当用户进程调用read系统调用时，它会阻塞等待内核将数据从文件描述符中读取到缓冲区中，然后将读取的数据返回给用户进程。

当内核接收到用户进程的read系统调用时，它会根据文件描述符中的文件类型和当前文件偏移量等信息，从相应的设备或文件中读取数据，并将数据写入到用户进程提供的缓冲区中。如果需要等待数据到达，内核会将用户进程置于阻塞状态，直到有足够的数据到达或者发生错误才返回。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838656575766f38317a6a333137303074613431302e6a7067.jpeg" alt="截屏2022-11-23 01.19.07" style="zoom:33%;" />



另外，一些设备驱动程序也可以通过向用户进程发送信号或者触发回调函数等方式，内核发起read操作。例如，网络套接字中的recv函数就是由内核发起的read操作，用于从网络中读取数据并将其返回给用户进程。



# 计算机网络

## 通用

### OSI七层模型【3】和五层模型？

```
应用层：进程间的通信和数据交互。为用户提供接口和服务
表示层：数据处理（编码、加密等）
会话层：管理通信会话
传输层：端到端的通信连接
网络层：数据路由
数据链路层：管理相邻节点之间的网络通信
物理层：传输介质
```

**七层模型**

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683865673874703168706a33307a713069616e30662e6a7067.jpeg" alt="截屏2022-11-23 02.07.13" style="zoom:33%;" />



<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030386933736b4e6c793167786b696839666e38706a33313371306a613077642e6a7067.jpeg" alt="截屏2021-12-20 19.14.11" style="zoom:33%;" />





**五层模型**

- 应用层：应用层是网络协议的最高层，主要任务**通过进程间的交互完成特定网络应用**。应用层协议定义的是 **应用程序（进程）间通信和交互的规则**。

  应用层协议有 HTTP、FTP、Telnet、DNS、SMTP等。应用层交互的数据单元称为 **报文**。

- 传输层：它负责**为两台主机中的进程提供通信服务**。该层主要有以下两种协议：

  - 传输控制协议 (Transmission Control Protocol，TCP)：提供**面向连接的、可靠的**数据传输服务，数据传输的基本单位是报文段（segment）；拥有流量控制、超时重传、拥塞控制等特性。
  - 用户数据报协议 (User Datagram Protocol，UDP)：提供**无连接的、尽最大努力的**数据传输服务，但不保证数据传输的可靠性，数据传输的基本单位是用户数据报。

- 网络层：网络层负责**为分组网络中的不同主机提供通信服务**，**并通过选择合适的路由将数据传递到目标主机**。在发送数据时，网络层把运输层产生的报文段或用户数据封装成 **分组** 或 包 进行传送。在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫 **IP数据报**。

  IP 协议的寻址作用是告诉我们去往下一个目的地该朝哪个方向走，路由则是根据「下一个目的地」选择路径。

- 数据链路层：数据链路层通常简称为 链路层。数据链路层在两个相邻节点传输数据时，**将网络层交下来的IP数据报组装成帧，在两个相邻节点之间的链路上传送 帧。**

- 物理层：保证数据可以在各种物理媒介上进行传输，**为数据的传输提供可靠的环境**。



**TCP/IP的体系结构**

应用层、传输层、网络层..

网络接口层：在 IP 头部的前面加上 MAC 头部，并封装成数据帧（Data frame）发送到网络上。MAC 头部是以太网使用的头部，它包含了接收方和发送方的 MAC 地址等信息，我们可以通过 ARP 协议获取对方的 MAC 地址。

网络接口层主要为网络层提供「链路级别」传输的服务，负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。



### 💧各层分别对应了哪些协议？【2】

```
应用层:HTTP、HTTPS、FTP、SMTP、DNS等。网络应用服务的协议。
表示层:JPEG、MP3、MPEG等。对数据进行翻译、加密和压缩。
会话层:SMB、NetBIOS、PPTP、SIP等。建立和管理会话。
传输层:TCP、UDP、RTP等。提供端对端的连接和可靠传输。
网络层:IP、IPv4、IPv6、IPSec、ICMP等。确定路由和寻址,实现不同网络间的通信。
数据链路层:Ethernet、PPP、HDLC、帧中继等。定义了MAC地址、链路建立/管理等。
物理层:Ethernet、Wi-Fi、USB、蓝牙等。定义了物理介质和传输信号的标准。
```





#### 802.3x 工作在几层，为什么【1】

数据链路层

1. 802.3x规定了一种流量控制的机制,可以暂停和恢复网络节点间的数据传输,防止缓冲区溢出和数据丢包。这属于数据链路层的功能。
2. 它的流量控制是点对点的,通过PAUSE帧实现。这需要工作在两个直接相连的网络节点之间,也就是链路层



流量控制用于防止在端口阻塞的情况下丢帧，这种方法是当发送或接收缓冲区开始溢出时通过将阻塞信号发送回源地址实现的。





## http

### http

#### 浏览器输入url到页面显示的过程【4】

https://blog.csdn.net/cute_ming/article/details/124364783

简洁版：

1. 输入url

2. DNS解析（解析过程见下一节）

   查询的两种方式：递归和迭代

3. 建立TCP连接

4. 客户端发送HTTP报文

5. 服务器响应请求，返回HTTP报文

6. TCP四次挥手断开连接

7. 浏览器解析文档资源并渲染页面

8. 浏览器发送请求获取其他在页面中的资源



详细版：

1. 用户在浏览器中输入网址

   当用户在浏览器中输入网址时，浏览器会检查输入的网址是否已存在于缓存中。如果已存在，则浏览器会从缓存中获取网页并显示给用户。如果不存在，则进行DNS解析，通过递归和迭代，查询域名到IP地址的映射。

2. 浏览器通过IP地址与目标服务器建立TCP连接，进行三次握手。

3. 浏览器向服务器发起HTTP请求

   浏览器将构建一个HTTP请求报文，并将其发送到服务器。HTTP请求通常包括请求方法（GET、POST、PUT等）、请求头（包含了浏览器的类型、语言偏好、支持的编码方式、缓存等信息）、请求体（包含了请求参数、请求体内容等信息）

4. TCP/IP协议传输数据

   HTTP请求被拆分成多个数据包，每个数据包被封装成TCP数据包。TCP数据包包括源IP地址和端口号、目标IP地址和端口号、序列号、确认号等信息。这些信息确保了数据包在网络上被正确发送和接收。

5. 路由器转发数据

   数据包从浏览器出发后，会通过一系列路由器的转发最终到达目标服务器。每个路由器都会查看数据包中的目标IP地址，并将其转发到适当的下一个路由器或最终目标服务器。

6. 服务器接收请求

   当数据包到达服务器时，服务器会查找数据包的目标IP地址，并使用**ARP（地址解析协议）**来查找目标服务器的MAC地址（物理地址）。这个过程称为地址解析，它确保了数据包被正确地发送到目标服务器的网络接口。一旦请求到达服务器，通过TCP协议将数据包重新组装成HTTP请求。服务器将解析请求报文，并使用相应的处理程序。

7. **服务器端处理请求**

   - 验证和解析请求报文：服务器首先会验证请求报文是否符合 HTTP 协议的规范，并对其进行解析，以便于后续处理。
   - 路由和处理请求：根据请求的 URL 和 HTTP 方法，服务器将请求路由到相应的处理程序，并进行处理。这个处理程序可以是一个后端服务，也可以是一个特定的功能或者数据的处理方法。
   - 数据库操作：当请求需要对数据库进行操作时，服务器会通过连接池等方式从数据库连接池中获取一个数据库连接，然后执行相应的 SQL 语句。
   - RPC 调用：当请求需要调用远程服务时，服务器会使用 RPC 框架进行远程调用，例如使用 gRPC、Thrift 等框架。
   - 消息队列：**当请求需要异步处理时，服务器会将请求发送到消息队列中，然后返回一个异步处理结果的响应**。这个响应可能是一个任务 ID 或者其他标识符，客户端可以通过这个标识符查询异步处理的结果。
   - 认证和授权：服务器在处理请求时可能需要对用户进行**认证和授权**，以保证请求的安全性。这通常涉及到用户的身份验证和授权信息的校验。
   - 缓存处理：服务器在处理请求时，如果请求的数据已经被缓存，则可以直接返回缓存中的数据，而不需要去查询数据库或者进行其他操作。
   - 日志和统计信息：服务器通常会将请求的处理过程以及处理结果记录到日志文件中，以便于后续的故障排除和性能调优。

8. 服务器生成响应

   响应通常包含一个HTML文件和其他相关文件，例如CSS文件和JavaScript文件。HTTP响应通常包括响应头（包含了服务器的类型、响应时间、响应状态码、缓存等信息）、响应体、状态行。

   - 状态行包含了HTTP协议版本号、响应状态码和响应状态信息。响应状态码用来表示请求处理的结果，比如200表示请求成功，404表示未找到请求资源，500表示服务器内部错误等。
   - 响应头包含了一些元信息，比如Content-Type，用来指定响应正文的数据类型，Content-Length，用来指定响应正文的长度，Cache-Control，用来指定缓存策略等。
   - 响应正文包含了请求所需要的数据，可以是文本、HTML、XML、JSON等格式的数据。当浏览器接收到响应后，它会根据响应报文中的元信息和响应正文来渲染页面并显示给用户。

   一旦响应生成完成，服务器将其发送回浏览器。HTTP响应数据也被拆分成多个TCP数据包，在网络中传输，通过路由器等网络设备转发，直到到达用户的计算机。

9. 浏览器渲染页面

   一旦浏览器收到响应，它将根据响应报文中的内容开始渲染页面。浏览器将解析HTML文件，并通过下载其他文件，例如CSS文件和JavaScript文件，来构建完整的页面。

   如果响应报文中包含的是一个HTML文件，浏览器会按照HTML标准来解析HTML代码，并将HTML代码转换为DOM树。浏览器在解析HTML代码的过程中，会遇到各种标签、属性、事件等，浏览器会按照HTML标准来解析它们，并执行相应的操作。当DOM树构建完成后，浏览器会根据CSS样式来对DOM节点进行样式计算，并将计算后的样式应用到DOM节点上，最终形成渲染树。渲染树上的每个节点都包含了它自己的样式、尺寸和位置等信息，浏览器会根据这些信息将渲染树绘制到屏幕上，完成页面渲染的过程。

   一旦页面完成构建，浏览器将其呈现给用户。



#### 💧扫二维码到进入页面的过程发生了什么？分别从 web、服务器、和用户端来说明。

1. **用户端**:
   - **扫描二维码**: 用户使用手机上的二维码扫描应用扫描二维码。二维码通常包含一个 URL。
   - **打开 URL**: 扫描应用识别出二维码中的 URL，然后打开浏览器并导航到该 URL。
2. **Web**:
   - **发送请求**: 浏览器发送一个 HTTP GET 请求到服务器。请求的 URL 就是二维码中的 URL。
   - **接收响应**: 服务器处理请求并返回响应。响应通常包含 HTML 页面，浏览器接收响应并渲染页面。
3. **服务器端**:
   - **接收请求**: 服务器接收到来自浏览器的 HTTP GET 请求。
   - **处理请求**: 服务器处理请求。这可能涉及从数据库获取数据、执行业务逻辑等。
   - **发送响应**: 服务器返回一个 HTTP 响应给浏览器。响应通常包含一个 HTML 页面。



#### 分别介绍 http 1.1 2.0 3.0【1】

**http 1.1**

HTTP/1.1 相比 HTTP/1.0 性能上的改进：

- 使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。
- 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。（解决了请求的队头阻塞）

但 HTTP/1.1 还是有性能瓶颈：

- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；
- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；
- **服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据**，也就是<font color=blue>队头阻塞</font>；
- 没有请求优先级控制；
- 请求只能从客户端开始，服务器只能被动响应。



**http 2.0**

HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。

那 HTTP/2 相比 HTTP/1.1 性能上的改进：

- 头部压缩

  如果同时发出多个请求，他们的头是一样的或是相似的，那么，协议会消除重复的部分。

  这就是所谓的 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。

- 二进制格式

  全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）。

- 并发传输

  **引出了 Stream 概念**，多个 Stream 复用在一条 TCP 连接。针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，**不同 Stream 的帧是可以乱序发送的**，**因此可以并发不同的 Stream** ，也就是 HTTP/2 可以并行交错地发送请求和响应。

- 服务器主动推送资源

  改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应。

  客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号



缺陷：

**TCP层的队头阻塞问题**

HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683835796730787330786a33306c73303777676d372e6a7067.jpeg" alt="截屏2022-11-15 17.46.51" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383579676d627372386a333067343064376d78772e6a7067.jpeg" alt="截屏2022-11-15 17.47.25" style="zoom:50%;" />

一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。



**http 3.0**

**HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP**，基于 UDP 的 `QUIC` 协议 可以实现类似 TCP 的可靠性传输

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683835796d6a32667a376a33306c67306139676d6f2e6a7067.jpeg" alt="截屏2022-11-15 17.53.05" style="zoom:50%;" />

QUIC 有以下 3 个特点。

- 无队头阻塞

  QUIC 连接上的多个 Stream 之间并没有依赖。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题

- 更快的连接建立

  对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，需要分批次来握手，先 TCP 握手，再 TLS 握手。

  QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以**「同时」完成建立连接与密钥协商**

  <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383631376b653335366a33306d383063737134322e6a7067.jpeg" alt="截屏2022-11-15 19.22.30" style="zoom:50%;" />

  （甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。）

- 连接迁移

  基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。

  QUIC通过**连接 ID**来标记通信的两个端点，只要保有上下文信息（比如连接 ID、**TLS 密钥**等），就可以复用原连接，消除重连的成本。



#### QUIC【2】

QUIC（Quick UDP Internet Connections）是一种基于UDP协议的网络传输协议，由谷歌公司开发。QUIC协议主要能够提供以下几个优点：

1. 快速连接：

   - 0-RTT：
     - **首次连接建立**：在首次与服务器连接时，客户端和服务器会协商一组共享的密钥和一些特定的会话参数。服务器会提供一个叫作“Session Ticket”的凭证，其中包括用于将来连接的所需信息。
     - **再次连接时使用0-RTT**：在后续的连接中，客户端可以使用先前获得的Session Ticket来重新构建必要的密钥材料，并使用这些密钥来加密第一波数据。此时，数据可以在握手完成之前立即发送，从而实现0-RTT连接建立。
   - 1-RTT：
     - **客户端发送ClientHello消息**：客户端向服务器发送一个ClientHello消息，其中包括支持的协议版本、密码套件、密钥交换参数等。
     - **服务器响应ServerHello消息**：服务器接收ClientHello并响应ServerHello消息。在此消息中，服务器选择协议参数并生成临时密钥。
     - **密钥协商**：客户端和服务器使用Diffie-Hellman密钥交换或类似的协议计算共享密钥。
     - **握手完成**：一旦共享密钥被计算出来，就可以建立双向安全通信。此时，双方可以开始发送加密的应用数据。

2. 安全性：QUIC都会采用 TLS 1.3 进行加密。

3. 多路复用：QUIC协议允许多个请求同时在一个连接上进行传输，因此，它能够更有效地处理HTTP队头阻塞的问题。QUIC协议允许同一连接中可以发起不同的Stream ID进行数据传输。

4. 前向纠错：QUIC协议采用前向纠错技术，这样就能够避免由于网络传输中数据包的丢失而引起的重传时间延迟问题。

   在传输过程中，如果某个数据包丢失或错误，接收方不会立即请求发送方重新发送数据包，而是使用前向纠错技术，**自动对下一个数据包进行纠错，直到错误的数据包到达为止**。这样，QUIC协议避免了传统TCP协议因为错误包的丢失而导致等待重传时间延迟的问题。

   前向纠错技术本质上是一种通过在数据包中添加一些额外冗余信息的方式来检测和纠正错误的方法。QUIC协议中的前向纠错技术采用了一些轻量级的公共校验码，将数据划分为一些小的数据块，然后将这些小块叠加在原始数据上并发送。**接收方则可以使用这些校验码来独立地检查这些数据块**，以确定那些数据出现了错误，并进行纠正。





#### http 状态码【1】

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/HTTP/6-%E4%BA%94%E5%A4%A7%E7%B1%BBHTTP%E7%8A%B6%E6%80%81%E7%A0%81.png" alt=" 五大类 HTTP 状态码 " style="zoom:50%;" />

`1xx` 类状态码属于**提示信息**，是协议处理中的一种中间状态，实际用到的比较少。

`2xx` 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态。

- 「**200 OK**」是最常见的成功状态码，表示一切正常。如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据。
- 「**204 No Content**」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。
- 「**206 Partial Content**」是应用于 HTTP 分块下载或断点续传，**表示响应返回的 body 数据并不是资源的全部**，而是其中的一部分，也是服务器处理成功的状态。

`3xx` 类状态码表示客户端请求的资源发生了变动，**需要客户端用新的 URL 重新发送请求获取资源**，也就是**重定向**。

- 「**301 Moved Permanently**」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。
- 「**302 Found**」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。

301 和 302 都会在响应头里使用字段 `Location`，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。

- 「**304 Not Modified**」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是**告诉客户端可以继续使用缓存资源**，用于缓存控制。

`4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义。

- 「**400 Bad Request**」表示客户端请求的报文有错误，但只是个笼统的错误。
- 「**403 Forbidden**」表示服务器禁止访问资源，并不是客户端的请求出错。
- 「**404 Not Found**」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。

`5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码。

- 「**500 Internal Server Error**」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。
- 「**501 Not Implemented**」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。
- 「**502 Bad Gateway**」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。
- 「**503 Service Unavailable**」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思





#### get和post请求的区别【2】

1. get请求一般是去取获取数据；post请求一般是去提交数据。
2. get因为参数会放在url中，所以隐私性，安全性较差，请求的数据长度是有限制的；post请求是没有的长度限制，请求数据是放在body中；
3. get请求刷新服务器或者回退没有影响，post请求回退时会重新提交数据请求。
4. get请求可以被缓存，post请求不会被缓存。
5. get请求会被保存在浏览器历史记录当中，post不会。get请求可以被收藏为书签，因为参数就是url中，但post不能。它的参数不在url中。
6. get请求只能进行url编码（appliacation-x-www-form-urlencoded）,post请求支持多种（multipart/form-data等）。



#### http 报文介绍【2】http的请求头包含哪些内容？【1】

HTTP报文分为请求报文和响应报文，两者都具有相同的结构。请求报文分为请求行、请求头部、空行和请求正文四个部分，其中请求头部包含了如下内容：

1. 请求方法（GET、POST、PUT、DELETE等）
2. 请求的URI（Uniform Resource Identifier）
3. HTTP协议版本号（HTTP/1.1）
4. 请求头部字段，包括：
   - Accept：浏览器可接受的MIME类型
   - Accept-Charset：浏览器能够处理的字符集
   - Accept-Encoding：浏览器能够处理的压缩算法
   - Cookie：服务器发送的cookie
   - User-Agent：浏览器或客户端的类型、版本号等信息
   - Referer：请求的来源
   - Host：请求的主机名
   - Content-Type：请求的数据类型（只有在POST请求中才有）
   - Content-Length：请求的数据长度（只有在POST请求中才有）
   - 客户端自定义请求头字段





#### http 报文头和报文体分隔符是什么

HTTP报文由报文头（headers）和报文体（body）组成，这两部分之间的分隔符是两个字符：回车（`\r`）和换行（`\n`），连续出现两次。这个序列通常被表示为`\r\n\r\n`。





### https

#### https的过程【2】（如何建立，交互了什么）

https://blog.csdn.net/weixin_44376670/article/details/116752225

TCP三次握手、TLS四次通信（TLS 1.3 改进成三次，TLS1.3舍弃了RSA的协商过程，然后基于ECDH的算法优化了整个过程。）

 ```
  SSL/TLS 协议基本流程：
  
  - 客户端向服务器索要并验证服务器的公钥。
  - 双方协商生产「会话秘钥」。
  - 双方采用「会话秘钥」进行加密通信。
 ```

TLS 协议建立的详细流程：

1. ClientHello

   客户端向服务器发起加密通信请求。发送以下信息：

   - 客户端支持的 TLS 协议版本，如 TLS 1.2 版本
   - 客户端生产的随机数（`Client Random`），后面用于生成「会话秘钥」条件之一。
   - 客户端支持的密码套件列表，如 RSA 加密算法（非对称加密）

2. SeverHello

   服务器收到客户端请求后，向客户端发出响应，回应的内容如下：

   - 确认 TLS 协议版本
   - 服务器生产的随机数（`Server Random`），也是后面用于生产「会话秘钥」条件之一。
   - 确认的密码套件列表，如 RSA 加密算法。
   - **服务器的数字证书。**

3. **客户端回应**

   确认服务器的数字证书的真实性，**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：

   - 一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。
   - 加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。
   - 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。

4. 服务器的最后回应

   收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。向客户端发送最后的信息：

   - 加密通信算法改变通知
   - 服务器握手结束通知

至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。





  



#### https一定安全吗？

HTTPS 协议本身到目前为止还是没有任何漏洞的，即使成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。

![截屏2022-11-15 17.30.46](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683835787a62737930726a33306c74306a646469312e6a7067.jpeg)

中间人攻击发生场景是有前提的，前提是<u>用户点击接受了中间人服务器的证书</u>。

中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。



#### 每次请求，https 的公私钥固定不变吗？【1】

(第一次会协商和加密，之后同一个浏览器的话会用 session id 做 session key 的提取，可以看做固定不变)







#### https 在传输消息前多出了几次 RTT(2-7次)【1】





#### 常见的系统端口号？【1】

22: SSH

25: SMTP

53: DNS

80: HTTP

111: RFC

443: HTTPS



#### http和https的区别【3】

明文/密文传输；连接建立；端口；数字证书验证身份

- HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。
- HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。
- 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。
- HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。





**HTTPS 解决了 HTTP 的哪些问题？**

HTTP 由于是明文传输，所以安全上存在以下三个风险：

- **窃听风险**，比如通信链路上可以获取通信内容
- **篡改风险**，比如强制植入垃圾广告，视觉污染
- **冒充风险**，比如冒充淘宝网站

HTTP**S** 在 HTTP 与 TCP 层之间加入了 `SSL/TLS` 协议，可以很好的解决了上述的风险：

- **信息加密**：交互信息无法被窃取
- **校验机制**：无法篡改通信内容，篡改了就不能正常显示
- **身份证书**



**HTTPS 是如何解决上面的三个风险的？**

- **混合加密**的方式实现信息的**机密性**，解决了窃听的风险。

  对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。

- **摘要算法**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。

  用摘要算法（哈希函数）来计算出内容的哈希值。

  <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838357772366e35366d6a33306d703061723734772e6a7067.jpeg" alt="截屏2022-11-15 16.48.20" style="zoom: 67%;" />

- 将服务器公钥放入到**数字证书**中，解决了冒充的风险。

  权威的机构就是 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。

  <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838357775396b63637a6a33306d7730673874616f2e6a7067.jpeg" alt="截屏2022-11-15 16.51.19" style="zoom:50%;" />





#### 为什么需要 非对称加密 和 对称加密？

对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。

非对称加密解决了密钥交换问题，但速度慢。







## DNS

#### DNS解析过程

1. 解析域名，浏览器查看（浏览器）缓存，再**查看 hosts 文件**，看看其中有没有和这个域名对应的规则，如果有的话就直接使用 hosts 文件里面的 ip 地址。

2. 如果在本地的 hosts 文件没有对应的 ip 地址，浏览器会**发出一个 DNS请求到本地DNS服务器** 。

3. 本地DNS服务器会首先**查询它的缓存记录**，如果缓存中有此条记录，就可以直接返回结果，此过程是**递归**的方式进行查询。如果没有，本地DNS服务器还要**向DNS根服务器进行查询**。

4. <font color=green>根DNS服务器</font>没有记录具体的域名和IP地址的对应关系，而是**告诉本地DNS服务器，你可以到顶级域名服务器上去继续查询**，并给出<font color=green>顶级域名服务器</font>的地址。这种过程是迭代的过程。

5. 本地DNS服务器继续向顶级域名服务器发出请求，在这个例子中，请求的对象是.com顶级域名服务器。.com顶级域名服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是**告诉本地DNS服务器，你的域名的权威域名服务器的地址**。

6. 最后，本地DNS服务器向对应的<font color=green>权威域名服务器</font>发出请求，这时就能收到一个域名和IP地址对应关系，**本地DNS服务器不仅要把IP地址返回给用户电脑**，还要把这个**对应关系保存在缓存中**，以备下次别的用户查询时，可以直接返回结果，加快网络访问。



#### DNS使用TCP还是UDP？

DNS占用53号端口，同时使用TCP和UDP协议。那么DNS在什么情况下使用这两种协议？

DNS在区域传输的时候使用TCP协议，其他时候使用UDP协议。

DNS区域传输的时候使用TCP协议：

1.辅域名服务器会定时（一般3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，会执行一次区域传送，进行数据同步。区域传送使用TCP而不是UDP，因为数据同步传送的数据量比一个请求应答的数据量要多得多。

2.TCP是一种可靠连接，保证了数据的准确性。

域名解析时使用UDP协议：

客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过三次握手，这样DNS服务器负载更低，响应更快。理论上说，客户端也可以指定向DNS服务器查询时用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。







## TCP/UDP

### TCP

TCP 是**面向连接的、可靠的、基于字节流的**传输层通信协议。



#### tcp与udp的区别【2】

连接建立、服务对象、可靠性、拥塞控制和流量控制、首部开销、传输方式、分片...

1. 连接
   - TCP 是面向连接的传输层协议，传输数据前先要建立连接。
   - UDP 是不需要连接，即刻传输数据。
2. 服务对象
   - TCP 是一对一的两点服务，即一条连接只有两个端点。
   - UDP 支持一对一、一对多、多对多的交互通信
3. 可靠性
   - TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达
   - UDP 是尽最大努力交付，不保证可靠交付数据。但是我们可以基于 UDP 传输协议实现一个可靠的传输协议，比如 QUIC 协议
4. 拥塞控制（慢启动，拥塞避免、快重传、快恢复）、流量控制（滑动窗口协议）
   - TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。
   - UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率
5. 首部开销
   - TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 `20` 个字节，如果使用了「选项」字段则会变长的。
   - UDP 首部只有 8 个字节，并且是固定不变的，开销较小。
6. 传输方式
   - TCP 是流式传输，没有边界，但保证顺序和可靠
   - UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。
7. 分片不同
   - TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片
   - UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。

TCP 和 UDP 应用场景：

TCP 经常用于：`FTP` 文件传输；HTTP / HTTPS；

UDP 经常用于：包总量较少的通信，如 `DNS` 、`SNMP` 等；视频、音频等多媒体通信；广播通信；



#### tcp为什么安全【1】

TCP在数据传输之前会有三次握手来进行连接

在数据传输时候，有确认、滑动窗口、超时重传、拥塞控制之类机制 

数据传输之后会进行四次挥手断开连接来节约系统资源。





#### tcp的超时重传机制【1】

发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 `ACK` 确认应答报文，就会重发该数据。

TCP 会在以下两种情况发生超时重传：

- 数据包丢失
- 确认应答丢失

`RTT` 指的是**数据发送时刻到接收到确认的时刻的差值**，也就是包的往返时间。

**超时重传时间 RTO 的值应该略大于报文往返 RTT 的值**。

RFC6289 建议使用以下的公式计算 RTO：

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383763726b643937326a33306c763067373075372e6a7067.jpeg" alt="截屏2022-11-16 22.47.51" style="zoom:50%;" />

其中 `SRTT` 是计算平滑的RTT ，`DevRTR` 是计算平滑的RTT 与 最新 RTT 的差距。

如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是**超时间隔加倍。**

也就是**每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。**



#### tcp快速重传

当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。





#### ⚠️tcp 和 udp 报文的区别(序号，确认号，数据偏移，标识符，窗口大小等)【1】







#### ⚠️tcp 可靠性，然后问十六位校验和怎么实现的【1】







#### tcp 粘包【2】

在组包过程中，把上一个包的内容与下一个包里的粘在了一起，被错误地当成了一个数据包解析了出来。

字节流是二进制数据，这些 01 串之间没有任何边界。

应用层传到 TCP 协议的数据，以字节流的方式发送到下游，**这些数据可能被切割和组装成各种数据包**，接收端收到这些数据包后没有正确还原原来的消息，因此出现粘包现象。



#### 不同协议能否监听同一个端口

可以。

传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。

传输层有两个传输协议分别是 TCP 和 UDP，**在内核中是两个完全独立的软件模块**。当主机收到数据包后，可以**在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP**，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。





### ⭐️三次握手

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383632667a777a6d626a33306c6c3068757766752e6a7067.jpeg" alt="截屏2022-11-15 20.05.12" style="zoom:50%;" />

#### 三次握手过程【N】

客户端和服务端都处于 `CLOSE` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态。

1. 客户端随机初始化序号（`client_isn`），将此序号置于 TCP 首部的「序号」字段中，同时把 `SYN` 标志位置为 `1` ，表示 `SYN` 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接。客户端处于 `SYN-SENT` 状态。
2. 服务端收到客户端的 `SYN` 报文后，首先服务端也随机初始化自己的序号（`server_isn`），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 `client_isn + 1`，接着把 `SYN` 和 `ACK` 标志位置为 `1`。最后把该报文发给客户端。之后服务端处于 `SYN-RCVD` 状态。
3. 客户端收到服务端报文后，首先该应答报文 TCP 首部 `ACK` 标志位置为 `1` ，其次「确认应答号」字段填入 `server_isn + 1` ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 `ESTABLISHED` 状态。

服务端收到客户端的应答报文后，也进入 `ESTABLISHED` 状态。





#### 为什么需要三次握手

1. 防止旧的重复连接初始化造成混乱【主要原因】

   > *The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.*
   >
   > -- RFC 793

   - 客户端连续发送多次 SYN 建立连接的报文，在网络拥堵情况下，「旧 SYN 报文」比「最新的 SYN 」早到达了服务端，服务端返回 SYN + ACK，ACK是旧的客户端序号+1
   - 客户端收到后 ACK 与期望不符，回复 RST 报文请求终止旧的连接
   - 服务端收到 <font color=blue>RST</font> 报文后，就会释放连接
   - 后续最新的 SYN 抵达了服务端后，客户端与服务端就可以正常的完成三次握手了。

   **在两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费**。

2. 同步双方初始序列号

   当客户端发送携带「初始序列号」的 `SYN` 报文的时候，需要服务端回一个 `ACK` 应答报文，表示客户端的 SYN 报文已被服务端成功接收

   那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，**这样一来一回，才能确保双方的初始序列号能被可靠的同步。**

   **两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。**

3. 避免资源浪费

   没有第三次握手，服务端不清楚客户端是否收到了自己回复的 ACK 报文，所以服务端每收到一个 SYN 就只能先主动建立一个连接。

   如果客户端发送的 SYN 报文在网络中阻塞了，重复发送多次 SYN 报文，那么服务端在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。



**为什么不是四次握手？**

服务端回复 ACK 和 发送 SYN 的过程可以合并。

三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。



⚠️**第1/2/3次握手丢失了，会发生什么？**





#### TCP SYN 攻击【1】

攻击者短时间伪造不同 IP 地址的 `SYN` 报文，服务端每接收到一个 `SYN` 报文，就进入`SYN_RCVD` 状态，但服务端发送出去的 `ACK + SYN` 报文，无法得到未知 IP 主机的 `ACK` 应答，久而久之就会**占满服务端的半连接队列**，使得服务端不能为正常用户服务。

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzI1LmpwZw?x-oss-process=image/format,png" alt="SYN 攻击" style="zoom:50%;" />



**TCP 半连接和全连接队列**

在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：

- 半连接队列，也称 SYN 队列；
- 全连接队列，也称 accept 队列；

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683837617971696464796a33306d71306631337a6c2e6a7067.jpeg" alt="截屏2022-11-16 21.45.31" style="zoom:50%;" />

正常流程：

- 当服务端接收到客户端的 SYN 报文时，会创建一个半连接的对象，然后将其加入到内核的「 SYN 队列」；
- 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；
- 服务端接收到 ACK 报文后，从「 SYN 队列」取出一个半连接对象，然后<font color=red>创建一个新的连接对象</font>放入到「 Accept 队列」；
- 应用通过调用 `accpet()` socket 接口，从「 Accept 队列」取出连接对象。

SYN 攻击方式最直接的表现就会把 TCP 半连接队列打满，这样**当 TCP 半连接队列满了，后续再在收到 SYN 报文就会丢弃**，导致客户端无法和服务端建立连接。



**避免 SYN 攻击方式**

- 调大 netdev_max_backlog；

  当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包，调大队列的最大值。

- 增大 TCP 半连接队列；

- 开启 tcp_syncookies；

  开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接，相当于绕过了 SYN 半连接来建立连接。

  <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683837623178783231386a33306d333067366a73672e6a7067.jpeg" alt="截屏2022-11-16 21.48.38" style="zoom:50%;" />

  - 当 「 SYN 队列」满之后，后续服务端收到 SYN 包，不会丢弃，而是根据算法，计算出一个 `cookie` 值；
  - 将 cookie 值放到第二次握手报文的「序列号」里，然后服务端回第二次握手给客户端；
  - 服务端接收到客户端的应答报文时，服务端会检查这个 ACK 包的合法性。如果合法，将该连接对象放入到「 Accept 队列」。
  - 最后应用程序通过调用 `accpet()` 接口，从「 Accept 队列」取出的连接。

- 减少 SYN-ACK 重传次数

  减少 SYN-ACK 的重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。







### ⭐️四次挥手

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383762346771726c6b6a33306c71306d357766792e6a7067.jpeg" alt="截屏2022-11-16 21.51.05" style="zoom:50%;" />

1. 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1` 的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态。
2. 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSE_WAIT` 状态
3. 客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态
4. 等待服务端处理完数据后，也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态。
5. 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态
6. 服务端收到了 `ACK` 应答报文后，就进入了 `CLOSE` 状态，至此服务端已经完成连接的关闭。
7. 客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSE` 状态，至此客户端也完成连接的关闭。

每个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。

<font color=red>**主动关闭连接的，才有 TIME_WAIT 状态**</font>



#### 为什么需要第四次挥手【3】

- 关闭连接时，客户端向服务端发送 `FIN` 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务端收到客户端的 `FIN` 报文时，先回一个 `ACK` 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 `FIN` 报文给客户端来表示同意现在关闭连接。

从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以**服务端的 `ACK` 和 `FIN` 一般都会分开发送，**因此是需要四次挥手。

（但是在特定情况下，四次挥手是可以变成三次挥手的）



**⚠️第1/2/3/4次挥手丢失了，会发生什么？**





#### timewait状态【6】

主动发起关闭连接的一方，才会有 `TIME-WAIT` 状态。

需要 TIME-WAIT 状态，主要是两个原因

- 防止历史连接中的数据，被后面相同四元组的连接 错误的接收；

  **足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**

- 保证「被动关闭连接」的一方，能被正确的关闭；

  > *TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.*
  >
  > --RFC 793

  **等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。**





**为什么time wait 等待时间是 2MSL**

https://blog.csdn.net/Whatssssup/article/details/115029996

`MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间。超过这个时间报文将被丢弃。

TIME_WAIT 等待 2 倍的 MSL。

1. 防止“已失效的连接请求报文段”出现在本连接中。使本连接持续的时间内所产生的所有报文段都从网络中消失，这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。

2. 为了保证主动关闭方的最后一个ACK报文段能够到达被动关闭方

   如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 `FIN` 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。

   （**2MSL时长** 这其实是相当于**至少允许报文丢失一次**。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。）



**timewait 过多怎么办**

- 增加可用端口数量: 默认情况下，系统可能只允许一定范围内的端口用于TCP连接。可以通过调整系统设置，增加可用端口的范围。

- 调整TIME_WAIT状态的持续时间: 缩短TIME_WAIT状态的持续时间。但是这样做可能会增加出现上述第2个问题的可能性。

- 重用TIME_WAIT连接: 通过调整系统设置，允许系统重用处于TIME_WAIT状态的连接。

- 负载均衡: 如果可能的话，可以通过增加服务器或者通过负载均衡的方式，分摊连接负担。



#### closewait状态【5】

**close wait状态是什么**

如果客户端是主动关闭连接的一方，当客户端发送FIN报文后，服务器会接收到这个FIN报文并发送ACK确认报文，表示已经接收到FIN报文。此时**服务端也需要做一些清理工作，如通知应用层读取完所有数据以及发送所有未发送的数据，等待关闭连接。**



**为什么要有close wait**

等待应用层在这个连接上的数据读取完毕，确认数据传输的完整性之后才发送FIN报文，避免引起应用层数据的丢失。





**如果出现了大量 close wait 状态，是因为什么呢？有什么解决办法？**

如果出现了大量CLOSE_WAIT状态，一般是因为连接的**主动关闭方没有及时关闭连接，或者被动关闭方没有发送ACK确认**，导致连接一直处于CLOSE_WAIT状态。

解决方法：

增加连接限制和超时设置，保证在连接关闭时及时关闭连接。

可以设置TCP的keepalive机制，在TCP连接长时间无数据交换时，自动发送检测数据包，防止连接长时间处于CLOSE_WAIT状态。







### UDP

### Socket编程

#### 了解socket编程吗，【2】其中accept方法是什么

- 服务端和客户端初始化 `socket`，得到文件描述符；
- 服务端调用 `bind`，将 socket 绑定在指定的 IP 地址和端口；
- 服务端调用 `listen`，进行监听；
- 服务端调用 `accept`，等待客户端连接；
- 客户端调用 `connect`，向服务端端的地址和端口发起连接请求；
- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符（新的，从全连接队列里取出一个新的连接对象）；
- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；
- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383763757a6a6f75746a33306d6d3069317439702e6a7067.jpeg" alt="截屏2022-11-16 22.51.08" style="zoom:50%;" />

监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。



**客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。**







#### ⚠️生成 tcp 和 udp socket有什么区别【1】

(在创建声明和 api 使用上均有区别)



#### socket 如何标识一个协议(sock_type)【1】

socket 的系统调用

```
int socket(int domain, int type, int protocal)
```

三个参数分别代表：

- domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；
- type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP；SOCK_DGRAM 表示的是数据报，对应 UDP；SOCK_RAW 表示的是原始套接字；
- protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；

根据创建 socket 类型的不同，通信的方式也就不同：

- 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM；
- 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM；
- 实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket；



### RPC

https://www.zhihu.com/question/524580708/answer/2584782720

RPC（Remote Procedure Call）是一种远程过程调用协议，它允许不同的进程间通过网络进行通信，像本地调用一样进行远程过程调用。**RPC协议定义了客户端和服务器端之间的通信格式，客户端通过调用远程方法（函数）并传递必要的参数，服务器端接收请求并返回结果。**

RPC 的工作流程大致如下：

1. 客户端调用本地的 stub（存根，可以认为是服务端的代理，存放服务端的地址等信息），传递调用所需的参数
2. 客户端的 stub 将参数序列化，并通过网络发送给远程服务端
3. 服务端接收到请求后，将请求数据反序列化为对应的参数，并调用本地的服务
4. 服务端执行完操作后，将结果序列化，并通过网络发送给客户端
5. 客户端接收到结果后，将结果反序列化为对应的值，并返回给调用者

RPC 的实现方式有很多，其中比较常用的是基于 HTTP 协议的 RESTful API 和基于 Google 开源的 Protobuf 协议。



#### 💧proto 有哪些好处？

- 高效: ProtoBuf 是二进制格式的，比 JSON、XML 等文本格式更高效，更小的体积，更快的速度。

- 强类型: ProtoBuf 使用接口定义语言 (IDL) 来定义消息类型、服务、枚举等，这使得它具有强类型的特点。

- 跨平台、跨语言: ProtoBuf 提供了多种语言的编译器，可以生成多种编程语言的代码。

- 向后兼容、向前兼容: ProtoBuf 设计了字段编号和默认值的概念，使得它具有良好的兼容性。

- 可扩展性: 很容易地扩展消息类型，而不需要修改旧的代码。





#### 💧http 和 RPC 有什么区别？

- 协议不同: HTTP 是一种应用层协议，用于传输超文本文档。RPC 是一种允许程序调用另一个地址空间（通常是在远程系统上）的过程或函数的协议。

- 格式不同: HTTP 消息通常是文本格式的，可以包含 HTML、JSON、XML 等多种格式的数据。RPC 消息通常是二进制格式的。

- 目的不同: HTTP 通常用于 Web 浏览器和 Web 服务器之间的通信。RPC 通常用于分布式系统中的进程间通信。

- 效率不同: HTTP 通信的开销比 RPC 大，因为 HTTP 消息包含很多额外的头信息。RPC 通常更高效，因为它使用二进制格式的消息。


注意: 在实践中，HTTP 和 RPC 通常可以互相转换。例如，可以使用 HTTP 作为 RPC 的传输层协议。例如，gRPC 是一个 RPC 框架，它使用 HTTP/2 作为传输层协议。





# 数据库/缓存

## Mysql

**执行一条 SQL 查询语句，期间发生了什么？**

- 连接器：建立连接，管理连接、校验用户身份；
- ~~查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。**MySQL 8.0 已删除该模块**；~~
- 解析 SQL，通过解析器对 SQL 查询语句进行**词法分析、语法分析，然后构建语法树**，方便后续模块读取表名、字段、语句类型；
- 执行 SQL：执行 SQL 共有三个阶段：
  - 预处理阶段：检查表或字段是否存在；将 `select *` 中的 `*` 符号扩展为表上的所有列。
  - 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
  - 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；

![截屏2022-11-17 19.00.45](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168383862746b706477336a333075633068656163352e6a7067.jpeg)

### ⭐️B+树【N】

https://mp.weixin.qq.com/s/w1ZFOug8-Sa7ThtMnlaUtQ

B+Tree 是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据是**按主键顺序存放**的。

每一层父节点的索引值都会出现在下层子节点的索引值中，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。



B+ 树与 B 树差异的点：

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。
- 非叶子节点中有多少个子节点，就有多少个索引





#### mysql的索引用的什么数据结构？为什么？原理是什么？【4】

MySQL 默认的存储引擎 InnoDB 采用的是 B+ 作为索引的数据结构，原因有：

- B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此**数据量相同的情况下**，相比存储即存索引又存记录的 B 树，**B+树的非叶子节点可以存放更多的索引**，减少内部节点的个数。因此 B+ 树可以比 B 树更扁平，**查询底层节点的磁盘 I/O次数会更少**。
- B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在**插入、删除的效率都更高**，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；
- B+ 树叶子节点之间用链表连接了起来，**有利于范围查询**，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。





#### 从数据页的角度看 B+ 树

采用页的方式存储数据记录，索引和数据分开存储。

InnoDB 里的 B+ 树中的**每个节点都是一个数据页**。

数据页中的记录按照「主键」顺序组成单向链表。

![截屏2022-11-18 20.25.42](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838396a7761356536736a3330716f306366676e302e6a7067.jpeg)

**页目录就是由多个槽组成的，槽相当于分组记录的索引**。然后，因为记录是按照「主键值」从小到大排序的，所以**我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**，无需从最小记录开始遍历整个页中的记录链表。





### ⭐️索引

索引的定义：帮助存储引擎快速获取数据的一种数据结构。



**索引的分类**

- 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**、跳表。
- 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。
- 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。
- 按「字段个数」分类：**单列索引、联合索引**。





#### 聚簇索引和非聚簇索引？【N】

聚簇索引的 B+Tree 和二级索引的 B+Tree 区别如下：

- 聚簇索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的 B+Tree 的叶子节点里；
- 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。

在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要<font color=blue>回表</font>，这个过程就是<font color=blue>覆盖索引</font>。如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。



InnoDB 在创建聚簇索引时，会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键；
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键；





#### 什么是回表？怎么减少回表？回表出现错误怎么办【1】

回表是指根据索引查出符合条件的记录后，需要再次访问数据页来获取完整的记录。



先检查二级索引中的 B+Tree 的索引值，找到对应的叶子节点，然后获取主键值，然后再通过主键索引中的 B+Tree 树查询到对应的叶子节点，然后获取整行数据。**这个过程叫「回表」，也就是说要查两个 B+Tree 才能查到数据**。



#### 减少回表的方法

1. 覆盖索引：**创建一个包含查询字段的复合索引**，这样查询时可以直接从索引中获取所需的全部字段，避免了回表操作。这种方式对于返回字段少、索引列和查询列相同的情况效果最佳。
2. 聚簇索引：聚簇索引是一种特殊的索引类型，它将数据存储在索引中，而不是在表中。因此，如果查询需要的字段都在聚簇索引中，就可以避免回表操作。但是，创建聚簇索引会对表的写入性能产生影响，因此需要根据实际情况进行考虑。
3. 列存储：列存储是一种将数据按列而非按行存储的方式，它可以提高查询性能和压缩比率，并减少回表的次数。这种方式在分析型应用中比较常用，例如数据仓库、BI等。
4. 冗余字段：如果查询的字段非常频繁，可以考虑将它们冗余存储在表中，这样可以避免回表的开销。但是这种方式需要考虑数据一致性和空间占用等问题。





**索引下推**

在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

索引下推的大概原理是：截断的字段不会在 Server 层进行条件判断，而是会被下推到「存储引擎层」进行条件判断，然后过滤出符合条件的数据后再返回给 Server 层。由于在引擎层就过滤掉大量的数据，无需再回表读取数据来进行判断，减少回表次数，从而提升了性能。





#### 覆盖索引和聚簇索引【2】

这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据。不需要回表。



#### 前缀索引

MySQL前缀索引是指对于一个列的前N个字符创建的索引。

通常情况下，MySQL会为列的每个字符创建一个索引，但如果数据量很大时，创建这样的索引会占用很多的空间和时间。

在创建前缀索引时，需要指定前缀长度。例如，如果需要创建一个长度为10的前缀索引，那么MySQL将只对每个列的前10个字符创建索引。这样可以有效地减少索引的大小，提高查询效率。但是，**由于只对部分字符创建索引，因此会存在一定的查询误差**。在实际应用中，需要根据具体的场景来权衡索引的大小和查询误差之间的关系，从而选择最合适的前缀长度。

需要注意的是，在使用前缀索引时，需要确保前缀的长度足够长，以避免过多的查询误差。另外，当需要根据索引进行排序时，前缀索引可能无法起到作用，因此需要使用完整的列索引。



#### 范围查找可以用索引吗？【1】

（可以，但具体执行要看优化器如何优化）

可以。

多个普通字段组合在一起创建的索引就叫做联合索引。

联合索引的最左匹配原则，在遇到范围查询（如 >、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。

注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配。（部分匹配）



 



#### 最左匹配原则。 怎么建立索引【2】

使用联合索引时，存在最左匹配原则，也就是按照最左优先的方式进行索引的匹配。

在使用联合索引进行查询的时候，如果不遵循「最左匹配原则」，联合索引会失效，这样就无法利用到索引快速查询的特性。

**利用索引的前提是索引里的 key 是有序的**。



#### 数据库中，如果对a、c建立联合索引，对b建立单列索引。如果查询条件是a和b，那么哪个索引会生效？【1】

只有b的单列索引会生效，联合索引(a, c)不会生效。因为在使用联合索引时，必须满足从左到右的顺序<font color=red>依次命中</font>索引列

而查询条件只包括a和b，而不包括c，因此无法使用联合索引。而b的单列索引可以被命中，可以加速查询。





#### 选择哪个字段作为索引？【1】

（面试官给了一个例子：查询一个student表，如果where条件中有性别和姓氏，应该选择哪个字段作为索引）

**建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**。

区分度就是某个字段 column 不同值的个数「除以」表的总行数：

$$
区分度 = \frac{distinct(column)}{count(*)}
$$
比如，性别的区分度就很小，不适合建立索引或不适合排在联合索引列的靠前的位置，而 UUID 这类字段就比较适合做索引或排在联合索引列的靠前的位置。

因为如果索引的区分度很小，假设字段的值分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是"30%"）很高的时候，它一般会忽略索引，进行全表扫描



#### 什么时候需要 / 不需要创建索引？

**什么时候适用索引？**

- 字段有唯一性限制的，比如商品编码；
- 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
- 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。



**什么时候不需要创建索引？**

- `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段
- 字段中存在大量重复数据。比如性别字段，只有男女。MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。
- 表数据太少
- 经常更新的字段不用创建索引。因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，影响数据库性能



#### ⚠️除了B+树以外还有什么数据结构可以作为索引【4】

https://ac.nowcoder.com/discuss/post/397921914335899648

MySQL 的 MyISAM 存储引擎支持多种索引数据结构，比如 B+ 树索引、R 树索引、Full-Text 索引。MyISAM 存储引擎在创建表时，创建的主键索引默认使用的是 B+ 树索引。

InnoDB 和 MyISAM 都支持 B+ 树索引，但是它们数据的存储结构实现方式不同。不同之处在于：

- InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身；
- MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址；



**FULLTEXT**

**按照分词原理建立索引的**。为了解决这类针对文本的模糊查询效率较低的问题。



**RTREE**

RTREE的优势在于范围查找。



**hash**



**跳表**





#### 什么情况下会索引失效？

- like 关键字左或者左右模糊匹配

  索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。

- 对索引使用函数

  因为索引保存的是索引字段的原始值，而不是经过函数计算后的值

- 对索引进行表达式计算

- 对索引隐式类型转换

  例如：如果索引字段是字符串类型，但是在条件查询中，输入的参数是整型，会进行全表查询。

  MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。

  （所以如果查询的是 where phone = 1111111111，MySQL 会把等号左边的 phone 索引转换成 int，即进行了表达式计算；

  如果查询的是 where id = "1"，MySQL 会把等号右边的 "1" 转换成 int，而左边的索引并没有受影响，所以此时索引没失效。）

- 联合索引非最左匹配

  在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。

- WHERE 子句中的 OR

  如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。



#### 怎么知道MySQL 有没有利用索引【2】

MySQL 可以通过执行 `EXPLAIN` 命令来查看查询执行计划，从而判断是否使用了索引。

具体操作步骤如下：

1. 打开 MySQL 命令行工具，连接到相应的数据库。
2. 在命令行中输入 `EXPLAIN`，接着输入你的查询语句，并以分号结束。
3. 执行完命令后，MySQL 会返回查询的执行计划。其中会显示**是否使用了索引、使用了哪个索引、索引的类型等信息。**（对应 type字段、keys 字段）





### 引擎

#### ⭐️InnoDB 和 MyISAM 的区别【6】

IInnoDB是MySQL中最常用的存储引擎，也是默认的存储引擎。它支持ACID事务、行级锁定、外键约束等特性，适用于大型数据库。

MyISAM是MySQL中较为简单的存储引擎，不支持事务和行级锁定，但它对于大量的SELECT操作非常快，适用于读密集型应用





1. 数据存储方式

   InnoDB存储引擎使用B+树索引，数据按照主键的顺序进行存储，因此InnoDB对于主键查询具有很高的性能，同时支持较快的范围查询和排序操作。

   MyISAM存储引擎则采用的是平衡树索引，数据存储在不同的物理文件中，因此在处理大量数据的情况下，MyISAM能够更快地进行查询和插入操作。

2. 事务支持

   **InnoDB存储引擎支持事务**，而MyISAM则不支持事务，因此在需要数据完整性和一致性的情况下，应该使用InnoDB存储引擎。

3. 锁机制

   InnoDB存储引擎采用**行级锁**，可以实现更高的并发性，同时还支持自适应哈希索引等功能，从而提升查询效率。

   MyISAM存储引擎则采用**表级锁**，因此在高并发情况下，可能会出现较多的锁等待时间，影响查询性能。

4. 崩溃恢复

   InnoDB存储引擎**支持崩溃恢复机制**，能够自动进行恢复操作，提高了数据库的可靠性。

   MyISAM存储引擎不支持崩溃恢复，如果数据库出现崩溃或数据损坏的情况，需要手动进行恢复操作。





### 事务

**事务的四个特性（ACID）**

- **原子性（Atomicity）**：一个事务中的所有操作，**要么全部完成，要么全部不完成**。事务在执行过程中发生错误，会被回滚到事务开始前的状态。
- **一致性（Consistency）**：是指事务操作前和操作后，数据满足**完整性约束**，数据库保持一致性状态。
- **隔离性（Isolation）**：数据库允许**多个并发事务同时对其数据进行读写和修改的能力**，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，**每个事务都有一个完整的数据空间，对其他并发事务是隔离的。**
- **持久性（Durability）**：**事务处理结束后，对数据的修改就是永久的**，即便系统故障也不会丢失。



**InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？**

- 原子性是通过 undo log（回滚日志） 来保证的；
- 持久性是通过 redo log （重做日志）来保证的；
- 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；
- 一致性则是通过持久性+原子性+隔离性来保证；





**事务并发问题**

- 脏读：读到其他事务未提交的数据；
- 不可重复读：前后读取的数据不一致；
- 幻读：前后读取的记录数量不一致。





**事务的隔离级别**【1】

- 读未提交（read uncommitted），指一个事务还没提交时，它做的变更就能被其他事务看到；
- 读提交（read committed），指一个事务提交之后，它做的变更才能被其他事务看到；
- 可重复读（repeatable read），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB 引擎的默认隔离级别；
- 串行化（serializable）；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，**如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成**，才能继续执行；

<img src="https://camo.githubusercontent.com/ef6221d958c3d3d98aa2c90a0f5c490adcceca902c065b7ed6de2da5677feac7/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838396c6f363179636d6a33306d383039676161622e6a7067" alt="截屏2022-11-18 21.27.08" style="zoom: 67%;" />

**MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了**）

解决的方案有两种：

- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。



**四个隔离级别如何实现？**

| 隔离级别 | 脏读   | 不可重复读 | 幻读   | 实现原理                                                     |
| -------- | ------ | ---------- | ------ | ------------------------------------------------------------ |
| 未提交读 | 可能   | 可能       | 可能   | 直接读取最新的数据                                           |
| 已提交读 | 不可能 | 可能       | 可能   | 在「每个语句执行前」都会重新生成一个 Read View               |
| 可重复读 | 不可能 | 不可能     | 可能   | 「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View。 |
| 可串行化 | 不可能 | 不可能     | 不可能 | 读写锁                                                       |







#### 说一下MVCC【4】

Read View 有四个重要的字段：

- m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的**事务 id 列表**，注意是一个列表，**“活跃事务”指的就是，启动了但还没提交的事务**。
- min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 **id 最小的事务**，也就是 m_ids 的最小值。
- max_trx_id ：这个并不是 m_ids 的最大值，而是**创建 Read View 时当前数据库中应该给下一个事务的 id 值**，也就是全局事务中最大的事务 id 值 + 1；
- creator_trx_id ：指的是**创建该 Read View 的事务的事务 id**。



聚簇索引记录中都包含下面两个隐藏列（在**undo log** 里）：

- trx_id，当一个事务对某条聚簇索引记录进行改动时，就会**把该事务的事务 id 记录在 trx_id 隐藏列里**；
- roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后**这个隐藏列是个指针，指向每一个旧版本记录**，于是就可以通过它找到修改前的记录。



在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况：

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838396d36626f6d31666a33306c793039796d79322e6a7067.jpeg" alt="截屏2022-11-18 21.44.33" style="zoom:50%;" />

- 如果记录的 trx_id 值小于 Read View 中的 `min_trx_id` 值，表示这个版本的记录是在创建 Read View **前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。
- 如果记录的 trx_id 值大于等于 Read View 中的 `max_trx_id` 值，表示这个版本的记录是在创建 Read View **后**才启动的事务生成的，所以该版本的记录对当前事务**不可见**。
- 如果记录的 trx_id 值在 Read View 的 `min_trx_id` 和 `max_trx_id` 之间，需要判断 trx_id 是否在 m_ids 列表中：
  - 如果记录的 trx_id **在** `m_ids` 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务**不可见**。
  - 如果记录的 trx_id **不在** `m_ids`列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务**可见**。

**这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。**





#### 幻读是什么，解决方案

幻读：前后读取的记录数量不一致。

- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为<font color=red>可重复读隔离级别</font>下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。



<!--Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。--> 

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683865686a757265726d6a333138363069386a73752e6a7067.jpeg" alt="截屏2022-11-23 02.52.25" style="zoom:50%;" />

**因为间隙锁的目的是防止插入幻影记录而提出的**。



<font color=blue>Next-Key Lock</font> 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。

（注意：区间是左开右闭！！！）

```
一致性和连续性： 在计算机科学中，索引一般是从零开始的，而不是从一开始。通过使用左开右闭的方式，可以保持索引从零开始的连续性。比如，对于数组的第一个元素，左端点是 0，而右端点是 1，依此类推，对于数组的第 n 个元素，左端点是 n-1，右端点是 n。

容易计算长度： 采用左开右闭的定义方式，临界区间的长度可以简单地通过右端点减去左端点来计算，而不需要进行加一操作，这在编程中会更加方便。

避免重叠问题： 在多线程或并发编程中，临界区间通常用于指定一段资源或数据的操作范围。使用左开右闭的定义方式，可以避免两个临界区间之间的重叠，因为右端点是一个临界值，不属于下一个区间。
```

![截屏2022-11-23 02.53.58](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683865686c67763476336a33313773306b386163362e6a7067.jpeg)

**next-key lock 即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中。**



#### MVCC能保证不产生幻读吗

仅仅依靠MVCC无法完全避免幻读的情况发送。

快照读是可以避免的，

对于当前读，如果没加间隙锁：

```
#事物A：
select name from user where id > 3;
#事物B：
insert into user valus('6','edwin');
#事物A：
update user set name = 'xxx' where id = 6;

```

事务A修改了事务B插入的数据，**update是当前读，所以此时会读取最新的数据(包括其他已经提交的事务)**

解决方案：记录锁+间隙锁



### 锁

**锁的分类**

- 粒度：
  - 全局锁
  - 表级锁
    - 表锁
    - 元数据锁
    - 意向锁
    - AUTO-INC 锁
  - 行级锁
    - Record Lock 记录锁
    - Gap Lock 间隙锁
    - Next-Key Lock 临键锁
    - 意向锁
- 方式：读锁（共享锁） / 写锁（排他锁、独占锁）
- 态度：悲观锁 / 乐观锁
  - 悲观锁 （读锁、写锁都是悲观锁）
  - 乐观锁 （乐观锁，需要外部程序实现）

（详细的介绍在 https://www.xiaolincoding.com/mysql/lock/mysql_lock.html#%E5%85%A8%E5%B1%80%E9%94%81）



#### MySQL 行级锁

**Record Lock**

Record Lock 称为记录锁，锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的：

- 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）;
- 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。



**Gap Lock**

Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；

Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是**为了解决可重复读隔离级别下幻读的现象**。

假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，**间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**。



**Next-Key Lock**

Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身

假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。

**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。



**插入意向锁**

一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。

如果有的话，插入操作就会发生**阻塞**，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个**插入意向锁**，表明有事务想在某个区间插入新记录，但是现在处于等待状态。





#### 了解Mysql的悲观锁和乐观锁吗？简单介绍一下【2】

https://blog.csdn.net/weixin_45433031/article/details/120838045

**悲观锁（Pessimistic Concurrency Control）**

在修改数据之前先锁定，再修改的方式。

悲观锁，具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度。因此，在整个数据处理过程中，将数据处于锁定状态。

**总是假设最坏的情况，每次读取数据的时候都默认其他线程会更改数据，因此需要进行加锁操作，当其他线程想要访问数据时，都需要阻塞挂起。**

悲观锁的实现：传统的关系型数据库使用这种锁机制，比如行锁、表锁、读锁、写锁等，都是在操作之前先上锁；Java 里面的同步 synchronized 关键字的实现

**悲观锁主要分为共享锁和排他锁**：

- 共享锁（shared locks）又称为读锁，简称 S 锁。共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。
- 排他锁（exclusive locks）又称为写锁，简称 X 锁。排他锁就是不能与其他锁并存，如果一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁。获取排他锁的事务可以对数据行读取和修改。



**乐观锁（Optimistic Locking）**

**乐观锁假设数据一般情况不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测**。乐观锁适用于读多写少的场景，这样可以提高程序的吞吐量。

乐观锁的实现：CAS （Compare And Set）：Java 中并发库**原子变量**使用了乐观锁的一种 CAS 实现方式；版本号控制：一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会 +1。当线程 A 要更新数据时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值与当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。

乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。



#### 如果删除索引出现问题了导致锁表怎么办？【1】

https://zhuanlan.zhihu.com/p/437702115

场景：

锁表通常发生在 DML（ insert 、update 、delete ）语句中，例如：程序 A 对 A 表的 a 数据 进行修改，修改过程中产生错误，没有 commit 也没有 rollback ，这个时候程序 B 对 A 表的 a 数据进行修改，会产生资源正忙的异常，也就是锁表。

DDL也会引发锁表，例如在 MySql 操作一张大表，利用 alter 语句修改或新增字段的时候，恰巧有一个长事务（包括读）在操作此表，会触发修改等待，造成锁表。

原因：

当多个事务处理对多个资源同时访问时，若双方已锁定一部分资源但也都需要对方已锁定的资源时，无法在有限的时间内完全获得所需的资源，就会处于无限的等待状态，从而造成其对资源需求的死锁，导致锁表。



解决方法：

1. 使用 SHOW PROCESSLIST 命令查看当前正在执行的查询。
2. 找到正在执行 DROP INDEX 或 ALTER TABLE 命令的查询。
3. 记下该查询的 id，使用 KILL 命令结束该查询的进程，例如 KILL id。需要注意的是，使用 KILL 命令结束进程会导致该进程被强制关闭，可能会造成数据损坏，所以必须慎重使用。





### 日志

**undo log（回滚日志）**

是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。

undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录**更新前的数据**到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。

每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，**要把回滚时需要的信息都记录到 undo log 里**

一条记录的每一次更新操作产生的 undo log 格式都有一个 **roll_pointer 指针和一个 trx_id 事务id**



undo log 两大作用：

- **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。
- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。







**redo log（重做日志）**

是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复；

当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次**对这个页的修改**以 redo log 的形式记录下来，**这个时候更新就算完成了**。InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**。

redo log 是物理日志，记录了某个数据页做了什么修改，每当执行一个事务就会产生这样的一条或者多条物理日志。在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。

系统崩溃重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。



redo log 的作用：

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**， MySQL 的写操作并不是立刻更新到磁盘上（随即写），而是先记录在日志上（顺序写），然后在合适的时间再更新到磁盘上 。提升 MySQL 写入磁盘的性能。





#### 数据库binlog,redolog,undolog的区别【3】

**redo log 和 undo log 区别？**

- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；
- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；

事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务。





**binlog （归档日志）**

是 Server 层生成的日志，主要用于数据备份和主从复制；

MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写入 binlog 文件。

binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作。







**redo log 和 binlog 有什么区别？**【3】

1. 适用对象不同

   binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；redo log 是 Innodb 存储引擎实现的日志

2. 文件格式不同

   binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED；redo log 是物理日志

3. 写入方式不同

   binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志；

   redo log 是循环写，日志空间大小是固定，保存未被刷入磁盘的脏页日志

4. 用途不同

   binlog 用于备份恢复、主从复制

   redo log 用于掉电等故障恢复





### 通用

#### ⭐️连接操作join了解吗？（左连接，右连接，内连接）

inner join(等值连接) 只返回两个表中联结字段相等的行


left join(左联接) 返回包括左表中的所有记录和右表中联结字段相等的记录

right join(右联接) 返回包括右表中的所有记录和左表中联结字段相等的记录



举例：

https://blog.csdn.net/Superman___007/article/details/90287101

https://www.cnblogs.com/lijingran/p/9001302.html



#### inner join连接操作怎么实现的？

https://blog.csdn.net/weixin_35950078/article/details/114328445

https://www.jianshu.com/p/14bb90a44216

**Simple Nested-Loop Join**

r为驱动表，s为匹配表，可以看到从r中分别取出r1、r2、......、rn去匹配s表的左右列，然后再合并数据，对s表进行了rn次访问，对 数据库 开销大

![09ef96b9763678372b7b509382ddc442.png](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/09ef96b9763678372b7b509382ddc442.png)

**Index Nested-Loop Join(索引嵌套)**

要求非驱动表(匹配表s)上有索引，可以通过索引来减少比较，加速查询。

在查询时，驱动表(r)会根据关联字段的索引进行查找，当在索引上找到符合的值，再回表进行查询，也就是只有当匹配到索引以后才会进行回表查询。

![94a7e525e6fa5336d0cfead7a1cb6218.png](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/94a7e525e6fa5336d0cfead7a1cb6218.png)



**Block Nested-Loop Join （无索引情况）**

是将驱动表的所有join相关的列都先缓存到join buffer中，然后**批量**与匹配表进行匹配，将第一种多次比较合并为一次，降低了非驱动表(s)的访问频率。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/webp.jpg" alt="img" style="zoom:50%;" />



**hash join**

1. 选择被哈希的表，<font color=red>通常是小一点的表</font>。假定是T1。
2. 然后扫描另一张表并使用散列函数在散列表中查找匹配的行。

时间复杂度O(n+m)，实现hash表是O(n)，hash表查找是O(m)，直接将其相加。



**merge join**

- 类似于归并排序的归并操作。通过同时遍历两个已排序的表，一次比较一对行，匹配则输出，否则移动指针。
- **优化**: 需要两个表已排序，或者至少在连接键上有序。如果已经有序或有适当的索引，则性能很好。

时间复杂度O(n*log(n)+m*log(m))。排序算法的复杂度分别是O(n*log(n))和O(m*log(m))









#### 不加索引怎么优化连接操作？

大表在前小表在后；分区分片；缓存；



#### 主从同步（异步，半同步）

主从同步是指在一个分布式系统中，将一个节点作为主节点，其他节点作为从节点，主节点对数据进行更新时，通过一定机制使得从节点能够及时地更新数据，保证数据一致性和可靠性的过程。

在主从同步中，常见的同步方式包括异步、半同步和全同步。

1. 异步复制 异步复制是指主节点**将更新的数据记录到日志中后，立即返回给客户端，然后再异步地将更新的数据同步到从节点**，由于从节点不需要等待主节点同步数据的过程，因此异步复制的性能很高，但是由于从节点不是实时同步主节点的数据，因此存在数据丢失的风险。
2. 半同步复制 半同步复制是指**主节点将更新的数据同步到至少一个从节点后，才会返回给客户端**，这样可以保证主节点和至少一个从节点的数据是一致的，减少了数据丢失的风险。但是由于主节点需要等待至少一个从节点同步数据的过程，因此相对于异步复制而言，性能略有降低。
3. 全同步复制 **全同步复制是指主节点将更新的数据同步到所有从节点后，才会返回给客户端**，这样可以保证所有节点的数据是一致的，数据安全性最高。但是由于主节点需要等待所有从节点同步数据的过程，因此性能相对较低。





#### 💧mysql 有什么加快查询的方法？如何进行性能优化？

MySQL数据库查询速度的优化是一个复杂而重要的任务，涉及多个方面的考虑。以下是一些常用的方法和技巧，可以加快查询速度并提高数据库性能：

1. **使用合适的索引**

   - **单列索引**：根据查询需求，对经常用于WHERE子句中的列创建索引。

   - **复合索引**：在多个列上创建索引，以便更有效地过滤记录。

   - **覆盖索引**：如果索引包含所有需要查询的字段，则查询只需在索引中完成，而无需访问实际表。

2. **优化查询语句**

   - 只选择需要的列，而不是所有列。

   - **使用JOIN代替子查询**：当可能的时候，JOIN通常比子查询更高效。

   - **避免在索引列上使用函数和运算**：这可能会导致索引失效。

   - **合理使用LIMIT**：如果不需要所有结果，则使用LIMIT限制返回的行数。


3. **规范化和反规范化设计**

   - **规范化**：通过消除重复数据，减小数据的大小，可以提高查询性能。

   - **反规范化**：通过添加冗余数据或预先计算的总结数据，也可以提高查询性能。


4. **使用合适的存储引擎**
   - 例如，InnoDB通常比MyISAM提供更好的并发支持。


5. **优化数据类型**
   - 使用最合适、最小的数据类型可以减小数据的存储空间，从而提高查询性能。


6. **缓存**

   - 利用MySQL的查询缓存功能，对经常访问的查询进行缓存。

   - 注意，某些场景下，查询缓存可能适得其反。需要根据实际情况进行评估。


7. <font color=red>**分区和分片**</font>
   - 通过分区或分片将大表拆分为更小、更易于管理的部分，从而提高查询性能。


8. **常规维护**

   - **定期分析和优化表**：保持数据存储的高效组织。

   - **监控和诊断工具**：使用如`EXPLAIN`计划、慢查询日志等来分析性能瓶颈。

9. <font color=red>**读写分离**</font>
   - 通过将读和写操作分离到不同的服务器，可以平衡负载并提高性能。




#### 💧EXPLAIN 的结果应该关注哪些？

```
1. type
- 这列显示了连接类型，通常是查询的核心部分。它可以告诉你MySQL是如何从表中找到行的。
- 值例如`const`、`eq_ref`、`ref`、`range`、`index`和`ALL`，其中`ALL`（全扫描）通常最糟糕，而`const`则最好。

2. possible_keys 和 key
- `possible_keys`列出了可能应用于查找的索引。
- `key`显示了实际选择使用的索引。
- 如果这些字段中没有索引，你可能需要考虑添加合适的索引以提高性能。

3. rows
- 这列显示了MySQL估计需要读取的行数。
- 该值越小越好，表示检索的数据量更少，性能可能更好。

4. filtered
- 这列显示了表条件的过滤百分比。
- 如果`filtered`百分比较低，表示大量数据被读取但未用于结果集，可能需要优化查询条件。

5. Extra
- 这列包含了关于查询执行计划的附加详细信息。
- 例如，“Using Filesort”表示MySQL需要对结果进行额外排序，“Using temporary”表示需要使用临时表等。
- 这些额外的步骤可能会对性能产生负面影响。

6. ref
- 这列显示了哪些列或常量用于与索引一起查找行。
- 如果`ref`包含了很多列，那么可能需要检查索引的选择和使用。

7. 查询的顺序
- EXPLAIN的结果按查询执行的顺序展示。有时候，改变JOIN的顺序可能会提高性能。
```









## ⭐️⭐️⭐️Redis（高并发被问麻了）

#### 为什么用redis？

1. 快速读写：Redis是基于内存的，因此可以非常快速地读写数据。
2. 支持多种数据结构：Redis支持多种数据结构，包括字符串、哈希、列表、集合和有序集合等，可以满足不同的需求。
3. 支持持久化：Redis支持将数据持久化到磁盘，以便在服务宕机后能够恢复数据。
4. 支持集群部署：Redis支持集群部署，可以横向扩展，提高系统的可伸缩性和容错性



### 数据结构

#### redis有哪些数据类型【3】

五种基本类型 （string, list, set, zset, hash） + 三种新类型（Bitmaps, HyperLoglog, Geographic）



#### ziplist

ZipList就是当【zset】和【hash】容器对象在元素个数较少或元素长度较短时采用的数据结构

需要满足以下两个条件：

- [key,value]键值对数量少于128个
- 每个元素的长度小于64字节



它是一块连续的内存空间，每一个元素都前后挨着，中间没有内存空隙。同时它也是一个经过特殊编码的双向链表，它的设计目标就是为了提高内存存储效率，

ZipList的Entry中记录前一个元素的长度是为了双向链表可以倒着遍历。

因为内存非常紧凑，所以新增元素时需要重新申请内存。



#### quicklist

`quicklist` 是一种双向链表和压缩列表结合的数据结构，用于存储列表（list）类型的数据。

`quicklist` 可以将一个大型的列表数据划分为多个小的压缩列表（`ziplist`），并将它们存储在一个双向链表中。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-10%2023.40.46.png" alt="截屏2023-05-10 23.40.46" style="zoom: 50%;" />





<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-10%2023.19.28.png" alt="截屏2023-05-10 23.19.28" style="zoom:50%;" />



#### zset【1】

https://blog.csdn.net/qq_33333654/article/details/127212040

https://mp.weixin.qq.com/s?__biz=MzU0OTE4MzYzMw==&mid=2247512204&idx=4&sn=8a2a24e139f1c5fdfcc0f017b280bd0e&chksm=fbb13972ccc6b0647387c63c87acb6f638ce8c99dca30d958a4e91ed766224f50a32ec7281ad&scene=27



zset在Redis中两种不同的实现，分别是zipList和skipList。

**zipList**

需要满足以下两个条件：

- [score,value]键值对数量少于128个；
- 每个元素的长度小于64字节；



**skipList**（hash和skipList）

- hash

  存储value到score的映射，查询时间复杂度为O(1)

- 跳表

  根据score的值排序后生成的一个跳表，可以快速按照位置的顺序或者score的顺序查询元素。

  skiplist按从小到大的顺序存储分数



**跳表的底层数据结构**

zskiplist

```
typedf struct zskiplist{
    struct zskiplistNode *header;  // 指向跳跃表的表头节点
    struct zskiplistNode *tail;  // 指向跳跃表的表尾节点
    unsigned long length;  // 记录跳跃表的长度，也就是跳跃表目前包含节点的数量（表头节点不计算在内）
    int level;  // 记录目前跳跃表内，层数最大的那个节点层数（表头节点的层数不计算在内）
}zskiplist;
```



zskiplistNode

- 层（level）：节点中用 L1、L2、L3 等字样标记节点的各个层，L1 代表第一层，L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其它节点，而跨度则记录了前进指针所指向节点和当前节点的距离。
- 后退（backward）指针：节点中用 BW 字样标识节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。
- 分值（score）：各个节点中的 1.0、2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。
- 成员对象（obj）：各个节点中的 o1、o2 和 o3 是节点所保存的成员对象。

```
typedf struct zskiplistNode{
    // 具体的数据
    sds ele;
    // 分数
    double score;
    //后退指针
    struct zskiplistNode *backward;
    
    //层级数组 最大32
    struct zskiplistLevel{  
        //前进指针forward
        struct zskiplistNode *forward;  // 用于访问位于表尾方向的其它节点
        //跨度span
        unsigned int span;  // 前进指针所指向节点和当前节点的距离
    }level[];
}zskiplistNode;
```





#### ⚠️Bitmaps

Bitmaps本身不是一种数据类型， 实际上它就是字符串(key-value) ， 但是它可以对字符串的位进行操作，字符串中每个字符对应1个字节

可以 把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在 Bitmaps中叫做偏移量。

![截屏2023-05-10 23.53.11](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-10%2023.53.11.png)



#### ⚠️HyperLoglog

HyperLogLog是一种基数估计算法，用于对海量数据中的不同元素进行去重和计数。

HyperLogLog算法的核心思想是**通过哈希函数将输入元素转换为一个二进制字符串，然后根据字符串中前缀0的个数来估计不同元素的数量。**

HyperLogLog算法的优势在于可以**使用固定的内存空间来处理海量数据，而且估计精度较高**。与传统的基数估计算法（如布隆过滤器）不同，**HyperLogLog算法的空间复杂度与元素数量无关，只与估计精度有关**。

HyperLogLog算法主要有三个组成部分：

1. 哈希函数：将输入元素转换为一个二进制字符串。
2. 桶（Bucket）：将哈希函数的输出结果分组为多个桶，并记录每个桶中前缀0的最大长度。
3. 估计器（Estimator）：通过桶中前缀0的最大长度来估计不同元素的数量，并进行修正以提高估计精度。

HyperLogLog算法的估计精度可以通过增加桶的数量来提高，通常可以达到0.01%的误差率。HyperLogLog算法已经被广泛应用于互联网领域中的数据去重和计数场景，例如网站的UV统计、广告点击量统计等。

**HyperLogLog只能统计基数的大小（也就是数据集的大小，集合的个数），它不能存储元素的本身，不能向set集合那样存储元素本身，也就是说无法返回元素。**

（原理我还没看）



#### 布隆过滤器

布隆过滤器（Bloom Filter）是一种快速、高效的数据结构，用于判断一个元素是否存在于一个集合中。它的主要作用是用于快速过滤掉不可能存在的元素，从而减少后续的查询和操作，提高系统的效率和性能。

布隆过滤器的核心思想是**利用一组哈希函数和一个位数组来表示一个集合**。具体地，对于一个元素，**通过多个哈希函数计算出多个哈希值，并将对应的位数组位置标记为1**。当查询一个元素时，同样通过多个哈希函数计算出多个哈希值，并检查对应的位数组位置是否都为1。如果有一位为0，则可以确定该元素不在集合中；如果所有位都为1，则该元素可能在集合中，需要进一步进行判断。

由于布隆过滤器使用了哈希函数和位数组，因此它具有以下特点：

- 空间效率高：以牺牲一定的精度为代价，可以使用少量的存储空间来表示大规模的数据集合；
- 查询效率高：查询一个元素的时间复杂度为O(k)，其中k为哈希函数的个数；
- 插入和删除操作不支持：由于每个元素的哈希值会影响其他元素的哈希值，因此无法进行删除操作，而插入操作需要重新计算哈希值并更新位数组。

布隆过滤器的应用场景包括：缓存、防止网络爬虫等非法访问、垃圾邮件过滤、DNA序列分析等。需要注意的是，布隆过滤器在使用过程中可能存在误判的情况，即判断一个元素不存在于集合中，但实际上该元素存在。因此，在使用时需要根据具体的场景进行调整和优化，以保证布隆过滤器的准确性和可靠性。



#### redis 如何处理大规模统计？【1】

1. Hash

   可以使用Redis的Hash来进行统计计数,Hash的field可以表示统计的度量指标,value就是对应的统计值。这样可以方便地存储和读取统计数据。

2. HyperLogLog

   Redis的HyperLogLog可以进行大规模数据的基数统计,只需要很小的内存就可以计算出结果。适合处理统计UV(独立访客数量)等问题。

3. Bitmap

   可以使用Redis的Bitmap来统计大规模的布尔信息,比如记录用户是否访问过,用1表示访问过,0表示未访问。采用压缩可以有效减少内存。
   
4. **数据分片和分布式处理：** 使用 Redis 集群或分片来分布式地存储和处理数据，以提高性能和可扩展性。



### redis 集群

#### 💧主从复制【2】

- slave 启动成功连接到 master 后，会给 master 发送数据同步信号
- master 接收同步消息后，会把主服务器的数据持久化到 rdb，同时收集接收到的用于修改的命令，master将rdb发送给slave，完成一次完全同步
- 全量复制：slave在接收到 master 的 rdb 文件后，将其存入磁盘并加载到内存
- 增量复制：master 继续将收集到的修改命令依次传给 slave，完成同步

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-11%2017.17.50.png" alt="截屏2023-05-11 17.17.50" style="zoom: 33%;" />

#### 一致性（用于解决分片问题）

Redis集群通过一种叫做“分区/分片”的机制来做到一致性。

在Redis集群中，所有的数据会被分成16384个分区（或者叫做槽）。每一个分区包含一部分键值对。每一个节点（服务器）负责一部分分区。

**每一个节点知道所有的其他节点和分区的情况。当一个客户端需要访问一个特定的键时，它可以询问任何一个节点来获取这个键所在的分区和节点的信息。**

这样，客户端就可以直接和负责这个键的节点通信，读取或者写入数据。

这种方式能够保证，对于一个给定的键，不管客户端询问哪个节点，它总是能获取到同样的分区和节点的信息。因此，所有的客户端都能看到一致的数据。

> 一致性哈希算法是一种常用的分片算法，它通过将数据和节点映射到一个固定的哈希环上，根据数据的哈希值来确定数据应该存储在哪个节点上。这样可以在增加或删除节点时，最小化数据的迁移量。







### 高并发

#### 分片【2】

<font color=red>数据分片旨在解决单台服务器无法处理大量数据和高并发访问的问题</font>，通过将数据分散到多台服务器上，以实现更好的性能、可扩展性和负载均衡。

在数据分片中，每个分片都是一个独立的数据库实例，拥有自己的存储空间、索引、内存缓存等。不同分片之间之间可以独立运行，可以分布在不同的物理服务器上。每个分片通常只存储一部分数据，通过合并多个分片的数据来实现对整个数据集的操作。



>假设你有一个超级大的数据库，每秒需要处理十万个请求。如果所有的请求都发送到同一个服务器，那么这个服务器可能会因为处理不过来而崩溃。但是，如果你将数据分片到10个服务器上，那么每个服务器每秒只需要处理一万个请求，负载就大大降低了。
>
>分片的主要优点是可以提高系统的可扩展性。当数据量和请求量增加时，你可以通过增加更多的服务器来分担负载。这就是为什么分片可以解决高QPS问题的原因。





#### ⭐️若 QPS 达到十万级别，如何确保 Redis 正常工作？

1. **分片**：如果单个Redis实例无法处理这么大的负载，可以考虑将数据分片到多个Redis实例上。这样，每个实例只需要处理一部分请求。
2. **Redis集群**：Redis集群是指将Redis数据分散存储到多个物理节点上，并且各个节点之间的数据和服务是共享的，从而实现了数据的高可用和自动化的负载均衡。Redis集群中的每个物理节点都是相同的，它们之间会进行数据复制和故障切换，从而保证节点之间的高可用性。（[集群和分片的区别](https://www.dbs724.com/147773.html)）
3. **连接池**：使用连接池可以复用已经建立的连接，避免频繁地创建和销毁连接，减少系统开销。
4. **使用缓存策略**：合理使用缓存策略，如 LRU（最近最少使用）策略，可以有效管理内存。





#### ⭐️redis 如何应对热点数据？

对于热点key的处理，Redis还提供了一些专门的方法：

1. 热点数据预热：在系统启动或负载较低的时候，可以提前将热点数据加载到Redis中进行缓存预热。这样可以避免系统刚启动或负载突然增加时，大量请求涌入导致热点key未被缓存的情况。
2. 热点数据不过期：可以设置热点数据的过期时间为永不过期，确保热点数据一直保持在缓存中，避免被淘汰。
3. 热点数据分片：将热点数据分散存储在多个Redis实例上，通过数据分片的方式来分担单个Redis实例的负载压力。这样可以提高系统的并发处理能力。







### 高可用


#### 有考虑过宕机的情况吗？

为了避免Redis服务宕机时数据的丢失，我们可以配置Redis进行持久化，即将数据定期写入磁盘中。Redis支持两种持久化方式：RDB和AOF。RDB是一种快照方式，将数据保存到磁盘中的一个文件中；AOF是一种追加方式，将每个写操作追加到一个日志文件中。

还可以采用Redis的主从复制机制，即将数据复制到多个Redis实例中，以提高系统的可用性和容错性。当主节点宕机时，从节点可以自动接管服务，从而避免数据的丢失和服务的中断。





#### 缓存穿透，缓存击穿，缓存雪崩

**缓存穿透**

一个请求进来后，会先从redis缓存中查询，缓存有就直接返回，缓存中没有就去db中查询。

有些key对应的数据在db中并不存在，每次 针对此次key的请求从缓存中取不到，请求都会压到db。



解决方法：

1. 对空值缓存

   一个查询返回的数据为空，把空值缓存，设置一个很短的过期时间

2. 设置白名单

   bitmaps类型定义一个可访问名单，不在名单里的进行拦截

3. 布隆过滤器

   将可能存在的数据哈希到一个足够大的 bitmaps中，一个一定不存在的数据会被 bitmaps拦截

4. 实时监控

   速度降低时，排查



**缓存击穿**

redis中某个热点key过期，此时大量请求到来，缓存没有命中，全部到了db上。db瞬时压力过大。



解决方法：

1. 预先设置热门数据，实时调整过期时间

2. 使用锁

   缓存中拿不到数据时，不是立即去db中查询，而是获取分布式锁（例如 setnx），拿到锁再去db中加载数据。没有拿到锁的线程休眠一段时间再尝试获取数据
   
3. <font color=red>分片，备份多个key</font>。（默认值返回或者空值返回对于线上来说，没什么用。访问不到就相当于失败）



**缓存雪崩**

key对应的数据存在，但是极短时间内有大量的key集中过期，此时若有大量请求，会落到db上，导致服务崩溃。

（缓存雪崩与缓存击穿区别在于，雪崩是大量的key集中过期，击穿是某个热点key过期）



解决方案：

1. 多级缓存

   nginx缓存+redis缓存

2. 使用锁

   保证不会有大量线程对数据库一次性进行读写

3. 监控缓存过期

4. 将缓存生效时间分散

   在原有失效时间基础上加一个随机值



#### 💧分布式锁

SET命令的NX和EX选项

使用SET命令与NX（只在键不存在时设置）和EX（设置键的过期时间）选项是Redis分布式锁的基础。例如，执行

```SET lock_key value NX EX 10```

将尝试获取名为lock_key的锁，只有当该键不存在时才设置它，并将其设置为10秒后过期。

为了避免进程在获取锁后崩溃并永久占用锁，Redis分布式锁通常设置有限的过期时间。如果锁的持有者在过期时间内未释放锁，则锁将自动过期并释放。

⚠️**底层实现？**





#### redis 持久化的方式(RDB, AOF)【2】

**RDB**

在指定的时间间隔内将内存中的**数据集快照写入磁盘**，它恢复时是将快照文件直接读到内存里。



持久化流程：

Redis会单独创建(fork)一个子进程进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束后，再用这个临时文件替换上次持久化好的文件。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-11%2016.55.53.png" alt="截屏2023-05-11 16.55.53" style="zoom: 33%;" />

优点：

- 适合大规模数据恢复
- 对数据完整性和一致性要求不高更适合使用
- 节省磁盘空间
- 恢复速度快



缺点：

- Fork时，内存中的数据会被克隆一份，大概2倍的膨胀
- 虽然在fork时使用了写时拷贝技术（等到修改数据时才真正分配内存空间），但是数据大时影响性能
- 周期备份，如果Redis意外崩溃，就会丢失最后一次快照后的所有修改



**AOF**

以日志的形式来记录每个写操作(增量保存)，将redis执行过的所有写指令记录下来(读操作不记录)，只允追加文件但不可改写文件。

redis启动之初会读取该文件重新构造数据，换言之，redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作



持久化流程：

- 客户端的请求写命令会被append追加到AOF缓冲区内
- AOF缓冲区根据AOF持久化策略（always, everysec, no）将操作同步到磁盘的AOF文件中
- AOF文件超过重写策略时，会对AOF文件进行重写，压缩AOF文件容量
- redis服务重启时，会重新加载AOF文件中的写操作达到数据恢复的目的



优点：

- 备份机制**更稳健**，数据丢失概率更低
- 可读的日志文本

缺点：

- 比RDB占用更多的磁盘空间
- 恢复备份速度更慢
- 每次读写都同步的话，有一定的性能压力



重写原理：

AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件，最后在rename替 换旧文件)，redis4.0版本后的重写，是指就把rdb的快照，以二进制的形式附在新的aof头部，作为已有的历史数据，替换掉原来的流水账操作。







### 💧Memcached

Memcached（Memory Cache Daemon）是一种开源的高性能分布式内存缓存系统。它被设计用于减轻数据库负载，提高应用程序的性能和响应速度。

Memcached的工作原理很简单：**它将数据存储在内存中，并使用键值对的方式进行访问。**当应用程序需要获取数据时，它首先会检查Memcached缓存，如果数据存在，则直接返回。如果数据不存在，则应用程序会从其他数据源（如数据库）中获取数据，并将其存储到Memcached缓存中，以供后续快速访问。



特点和优势：

1. 高性能：由于数据存储在内存中，Memcached可以提供非常快速的读写操作和低延迟的响应时间。它可以有效地减轻数据库的负载，提高应用程序的性能和吞吐量。
2. 分布式架构：Memcached支持分布式部署，可以在多台服务器上运行多个Memcached实例，以提供更大的存储容量和更高的并发性能。**它使用一致性哈希算法来分配数据到不同的节点上。**
3. 简单的键值存储：Memcached使用简单的键值对存储模型，使得数据的读写操作非常直观和简单。它支持各种数据类型，如字符串、整数、列表和集合等。
4. 可扩展性：由于Memcached的分布式架构，它可以轻松地扩展以适应不断增长的负载。通过添加更多的服务器节点，可以增加存储容量和处理能力。
5. 多语言支持：Memcached提供了多种编程语言的客户端库，如Python、Java、PHP等，使得开发人员可以方便地与Memcached进行交互。

需要注意的是，Memcached是一个缓存系统，不是一个持久化的数据存储解决方案。它适用于缓存频繁访问的数据，但不适合存储需要长期保存的数据。



#### 💧redis 与 memcached 对比

1. 数据类型支持：Redis支持更多的数据类型，包括字符串、哈希、列表、集合、有序集合等。而Memcached只支持简单的键值对存储。
2. 持久化支持：Redis支持数据的持久化，可以将数据保存到磁盘上，以便在重启后恢复数据。而Memcached不支持数据的持久化，数据仅存在于内存中。
3. 复杂性：Redis相对于Memcached来说更加复杂。Redis提供了更多的功能和灵活性，如发布/订阅、事务、Lua脚本等。而Memcached更加简单和轻量级，只专注于缓存功能。
4. 内存管理：Redis具有更高级的内存管理功能，可以根据配置设置最大内存使用量，并提供不同的淘汰策略来处理内存不足的情况。Memcached则没有内存管理功能，需要依赖操作系统来管理内存。
5. 分布式支持：Redis和Memcached都支持分布式部署，但它们的分布式机制略有不同。Redis使用主从复制和分片来实现数据的分布式存储，而Memcached使用一致性哈希算法来分配数据到不同的节点上。
6. 扩展性：Redis在扩展性方面更加灵活，可以通过添加更多的节点来增加存储容量和处理能力。而Memcached的扩展性相对较弱，需要通过增加更多的服务器来扩展。

综上所述，Redis适用于更复杂的应用场景，需要更多的功能和灵活性，而Memcached适用于简单的键值对缓存需求，追求更高的性能和简单性。选择哪个系统取决于具体的应用需求和优先考虑的因素。





## 通用

#### 💧数据库范式

```
- 1NF（第一范式）：
  定义：数据库表的每一列都包含原子、不可分割的值，每一行都有一个唯一的键。
  目的：消除重复的列，组织数据。

- 2NF（第二范式）：
  定义：表满足1NF，并且所有非键列完全依赖于整个候选键（而不是候选键的一部分）。
  目的：消除部分依赖，确保数据完整性。

- 3NF（第三范式）：
  定义：表满足2NF，并且所有的属性都完全函数依赖于主键，而不是其他非主键属性。
  目的：消除传递依赖，进一步减少冗余。

- BCNF（博伊斯-科德范式）：
  定义：表满足3NF，并且每一个非平凡的函数依赖项的超键是候选键。
  目的：解决3NF可能仍然存在的某些异常情况，进一步增强数据完整性。

- 4NF（第四范式）：
  定义：表满足BCNF，并且消除了多值依赖。
  目的：解决一些多对多关系的问题，确保数据独立性。

- 5NF（第五范式，也称为投影连接范式）：
  定义：确保不能通过分解和重新连接来重构表。
  目的：确保最高级别的数据完整性和一致性。

- 6NF（第六范式）：
  定义：允许对时间和空间进行完整描述，并提供历史数据管理功能。
  目的：提供对时间依赖数据的深入支持。
```





#### 数据库如何处理高并发写入（十万百万级）？

（缓存不是一个合适的建议，因为写入要求数据更新频繁，缓存的意义甚至与之相悖。）

<font color=red>分片、分布式</font>

同上面redis分散key的原理，数据库分库分表。

例如，分别写入到10份数据库，在流量低峰时合并。

在同一个数据库里，可以分别写入到10个数据表，在空闲时合并。



#### ⭐️redis 和 数据库的一致性

在读多写少的场景中，确保Redis和数据库的一致性是一个重要的问题。以下是一些常用的策略：

1. **读写都通过Redis**：所有的读写操作都先经过Redis，然后再异步地更新到数据库。这样可以保证Redis和数据库的数据一致性，但是如果Redis出现问题，可能会导致数据丢失。
2. **延迟写入**：当数据写入Redis时，同时将操作写入一个队列。然后有一个后台进程从队列中取出操作并更新到数据库。这样可以减少对数据库的直接写入操作，但是需要处理Redis和数据库更新的同步问题。
3. **读Redis，写数据库**：所有的写操作都直接写入数据库，然后更新Redis。读操作只读Redis。这样可以保证数据的一致性，但是如果写操作很多，可能会导致Redis和数据库的数据不同步。
4. **使用事务**：如果你的数据库支持事务，你可以在一个事务中同时更新Redis和数据库。这样可以保证数据的一致性，但是会增加系统的复杂性。
5. **使用TTL**：为Redis中的数据设置一个过期时间（TTL），当数据过期时，从数据库中重新加载。这样可以保证数据的一致性，但是可能会增加数据库的负载。







# 数据结构

### 哈希

#### 哈希索引【3】

哈希索引（hash index）基于哈希表实现，**只有精确匹配索引所有列的查询才有效**。

对于每一行数据，存储引擎都会**对所有的索引列计算一个哈希码（hash code）**，哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。**哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。**



**静态哈希**

散列索引将散列函数作用于搜索码以确定对应的桶， 然后**将此搜索码以及对应的指针存入此桶(或溢出桶)中。**

![静态哈希.png](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f73322e617831782e636f6d2f323031392f30352f32322f56397435616e2e706e67.png)

最大的缺点在于**必须在实现系统时选择确定的散列函数**。此后若被索引的文件变大或缩小，要想再改变散列函数就不容易了。



**动态哈希**

- 选择一个具有均匀性和随机性特性的散列函数 h。此散列函数产生的值范围相对较大，是 b 位二进制整数，一个典型的 b 值是 32。
- 把记录插入文件时按需建桶，用小于等于 b 的 i 个位用作附加桶地址表中的偏移量，i 的值会随着数据库大小的变化而增大或者减少。



#### hash为什么不能范围查找？

Hash操作并不能保证顺序性，所以值相近的两个数据，Hash值相差很远，被分到不同的桶中。

如何在使用Hash索引的同时，依然保留数据值的大小顺序：

1. **LRU缓存**

   在Hash索引的基础上，将所有数据通过链表有序的排列，每次添加新的值时有序插入，这样就可以执行范围查找了。但是这会让插入的复杂度变为O(n),效率低下
2. **通过Hash表和跳表实现有序集合**

   通过这种思路改造Hash索引也可以在支持范围查找的的同时将插入的复杂度降低到O（logN）。但是考虑到磁盘的IO影响，跳表并不能很好的控制IO的次数，平均IO次数也超过B+树，B+tree确实是最适合关系型数据库的索引结构。因此只有基于内存的Redis数据库使用了这种索引方式。



#### 减少Hash冲突的办法？【2】效率对比？假定冲突的次数固定为n

1. 链地址法。对于相同的哈希值，使用链表进行连接 => 在冲突的情况下，查找、插入和删除的时间复杂度为*O*(*n*)，其中*n*是链表中的元素数量。

2. 再哈希法。=> 效率与具体实现有关。

3. 建立公共溢出区。将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。

4. **开放定址法**

   当关键字key的哈希地址p 出现冲突时，以p为基础，产生另一个哈希地址p1，若p1仍然冲突，再次... 直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。

   - 线性探测 => 在*n*次冲突时，时间复杂度可能接近*O*(*n*)。
   - 平方探测 => 效率通常优于线性探测但仍可能高于常数时间。
   - 伪随机探测





#### 哈希和B+树查询数据的时间复杂度

（答了哈希O(1)，B+树$O(log_{m}n)$其实就是树高





#### 跳表怎么实现

使用二分查找的思想，对有序链表建立一级“索引”。 每两个节点提取一个节点到索引层。 索引层中的每个节点 都包含两个指针，一个指向下一个节点，一个down指针，指向下一级节点。

![截屏2022-11-23 12.32.41](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168386579626e383975356a33316a3230363474396f2e6a7067.jpeg)



### ⚠️ElasticSearch

https://blog.csdn.net/qq_43403025/article/details/114779166

**倒排索引(Inverted Index)**：倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。**倒排索引主要由两个部分组成：“单词词典”和“倒排文件”**。

- **单词词典(Lexicon)**：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，<u>单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。</u>
- **倒排列表(PostingList)**：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。
- **倒排文件(Inverted File)**：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683874663338746236716a333167773074676e33342e6a7067.jpeg" alt="截屏2022-12-06 00.50.39" style="zoom:30%;" />





### 排序

#### 拓扑排序

拓扑排序就是对于一个有向无环图中进行排序，大致的方法是：找出图中入度为零的点，接着依次将这些点和与点相关的边删去，同时更新其他点的入度；第二次再次查找入度为零的点……不断重复直到所有点都遍历完；



（模版题可参考**HDU 3342**）



#### 堆排序【2】

**堆排序基本思想**

- 将待排序序列构造成一个大顶堆，此时整个数组的最大值就是堆结构的顶端
- 将其与末尾元素进行交换，此时末尾就为最大值。
- 然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了。



**时间复杂度**

最坏，最好，平均**时间复杂度均为O(nlogn)**





**性质**

堆是具有以下性质的**完全二叉树**：每个**结点的值都大于或等于**其左右孩子结点的值，称为**大顶堆,** 注意 : 没有要求结点的左孩子的值和右孩子的值的大小关系。每个**结点的值都小于或等于**其左右孩子结点的值，称为**小顶堆**



**构造堆**

![截屏2022-11-23 12.55.27](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683865797a63353363716a333135323039676a74372e6a7067.jpeg)

时间复杂度是 o(n)



**维护堆**

删除根节点后，将最后一个节点置入根节点，此时先比较两个孩子找到最大的孩子结点，并与根节点进行比较，若大于根节点，则与根节点进行交换，之后再向下进行检查，直到满足堆的性质的时候停止检查。





#### 堆排序过程。时间复杂度。维护堆过程。堆中插入一个节点和删除一个节点的流程，时间空间复杂度。





#### 堆排序建堆过程，建堆时间复杂度





# Golang

#### 为什么选择Go？

1. 部署简单：编译后的程序是单一的可执行文件，部署非常简单。而Java程序需要配合JVM环境运行，部署相对复杂。
2. 简单易用的**标准库**：如网络编程、数据处理等
3. GC：自动垃圾回收



## 基础

### new和make

#### new和make有什么区别？【2】

（指针、slice 切片、管道 channel、接口 interface、map、函数等都是引用类型）

new()对类型进行内存分配,入参为类型,返回为**类型的指针**，指向分配类型的内存地址

make()也是用于内存分配的，但是和new不同，它**只**用于channel、map以及slice的内存创建和初始化，而且它返回的类型就是这三个**类型本身**。

```
func new(Type) *Type
func make(t Type, size ...IntegerType) Type
```



- make和new都是golang用来分配内存的内建函数，且在堆上分配内存，make 即分配内存，也初始化内存。new只是将内存清零，并没有初始化内存。
- make返回的还是引用类型本身；而new返回的是指向类型的指针。
- make只能用来分配及初始化类型为slice，map，channel的数据；new可以分配任意类型的数据。



#### make一个slice后返回的是指针吗？map呢？【1】

不是，返回的是slice类型本身



### Map

map 是 Golang 中一种键值对的数据结构，它类似于 Python 中的字典。map 中的键必须是可比较的类型，而值可以是任意类型。

**map 中的键值对是无序的，但是可以通过 range 关键字进行遍历**。

map 还支持动态增加和删除键值对。

 

#### map 不用make进行初始化，如果读写会发生什么【1】

如果只声明一个 map，而不使用 make 初始化，那么该 map 的值为 nil，**此时对该 map 进行任何操作都会引发运行时错误**，因为这个 map 并不存在。

"panic: assignment to entry in nil map" 或 "panic: runtime error: invalid memory address or nil pointer dereference" 





#### map是线程安全的吗？【2】

https://www.cnblogs.com/peteremperor/p/14469710.html

不是线程安全的。

以下是几种常见的保证 map 线程安全的方法：

- 互斥锁：可以使用 sync 包中的 Mutex 类型来保证 map 的互斥访问，即在读取和写入 map 时先获取锁，确保同一时间只有一个 goroutine 能够访问 map，从而避免竞态条件的发生。
- 读写锁：可以使用 sync 包中的 RWMutex 类型来实现读写锁，即在读取 map 时使用读锁，而在写入 map 时使用写锁
- 原子操作：可以使用 sync/atomic 包中的原子操作函数来实现对 map 中某个字段的原子更新操作，从而避免竞态条件的发生。
- 并发安全的 map：可以使用第三方的并发安全的 map 实现，如 GoConvey 中的 ConcurrentMap，或使用 sync.Map，它是 Golang 官方提供的并发安全的 map 实现。
- channel





### Slice

#### slice内部有什么？【1】

slice底层是一个struct。包含指向数组的指针、切片长度、切片当前容量。

```
// runtime/slice.go
type slice struct {
    array unsafe.Pointer// 指向数组的指针
    len   int
    cap   int
}
```



#### slice 和 array 区别【1】

**Array**

数组（Array）是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。因其**长度的不可变动**，数组在Go中很少直接使用。把一个大数组传递给函数会消耗很多内存。一般采用数组的切片。

**Slice**

Slice是一种数据结构，描述与Slice变量本身分开存储的Array的连续部分。 Slice不是Array。Slice描述了Array的一部分。

slice底层是一个struct。包含指向数组的指针、切片长度、切片当前容量。可以通过append追加元素。

```
// runtime/slice.go
type slice struct {
    array unsafe.Pointer// 指向数组的指针
    len   int
    cap   int
}
```



#### ⚠️【需要更新】slice的扩容【1】具体的计算过程【2】

slice 扩容的具体策略如下：

1. 当向 slice 中添加元素时，如果 slice 的长度小于容量的 2/3，那么 slice 不会扩容，而是直接在原有数组上添加元素。
2. 当向 slice 中添加元素时，如果 slice 的长度等于容量，那么 slice 将会扩容，如果原有容量小于 1024，则新的容量为原有容量的 2 倍；否则，新的容量为原有容量的 1.25 倍。
3. 扩容时，Golang 会申请一块新的内存作为底层数组，然后将原有数组中的元素复制到新的数组中，最后将 slice 的指针指向新的数组。在此过程中，如果原有数组包含的是非指针类型的数据，那么会进行一次值拷贝操作。
4. 在 slice 扩容后，原有的底层数组不会被立即释放，只有在原有数组中的元素被完全释放后，底层数组才会被垃圾回收器回收。





#### slice 在遍历过程中可以修改吗？【1】

可以。

1. 如果在遍历过程中添加元素，可能会导致当前遍历的元素被跳过或者重复访问，因为在添加元素后，slice 的长度会增加，而且新添加的元素可能会出现在当前遍历的范围之外。
2. 如果在遍历过程中删除元素，也可能会导致当前遍历的元素被跳过或者重复访问，因为在删除元素后，slice 的长度会减少，而且后面的元素会向前移动，从而改变当前遍历的索引和元素值。
3. 如果在遍历过程中修改元素值，虽然不会破坏遍历的正确性，但是可能会导致其他 goroutine 访问到不一致的数据，从而导致竞态条件和并发安全问题。因此，在多 goroutine 环境下，应该使用**互斥锁**等机制来保护共享数据的并发访问。





### 💧Channel

Go语言的`channel`是一种并发安全的通信机制，用于在不同的 goroutine 之间传递消息。它实际上是一个可以用来收发数据的队列，使用`make`函数创建，可以指定其容量和类型。

使用`channel`的主要优势在于其避免了使用锁的复杂性和低效性。当多个 goroutine 需要协作完成一个任务时，通过使用`channel`进行通信可以让代码更加简单、易读、可维护。



#### 底层结构

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/f0835ca71f0a5e5ba362d7e40351eded.png" alt="img" style="zoom:50%;" />

简要说明：

- buf是带缓冲的channel所特有的结构，是个循环链表，用来存储缓存数据

- sendx和recvx是用于记录buf中发送和接收的index
- lock是个互斥锁，目的是为了保证goroutine以先进先出FIFO的方式进入结构体
- recvq和sendq分别是往channel接收或发送数据的goroutine所抽象出来的数据结构，是个双向链表

```
type hchan struct {
    qcount   uint           // total data in the queue
    dataqsiz uint           // size of the circular queue
    buf      unsafe.Pointer // points to an array of dataqsiz elements
    elemsize uint16
    closed   uint32
    elemtype *_type // element type
    sendx    uint   // send index
    recvx    uint   // receive index
    recvq    waitq  // list of recv waiters
    sendq    waitq  // list of send waiters

    // lock protects all fields in hchan, as well as several
    // fields in sudogs blocked on this channel.
    //
    // Do not change another G's status while holding this lock
    // (in particular, do not ready a G), as this can deadlock
    // with stack shrinking.
    
    lock mutex
}
```





#### golang协程如何使用channel进行同步？

在使用 channel 进行同步时，可以利用 channel 的**阻塞特性**来进行协程之间的同步。例如，一个协程可以向一个无缓冲的 channel 发送数据，而另一个协程则可以从该 channel 中接收数据，这样就可以实现两个协程之间的同步。



1. 发送数据到 channel：使用 channel 的 send 操作可以将数据发送到 channel 中，如果 channel 已满，send 操作会阻塞当前协程直到 channel 中有空闲的位置。
2. 从 channel 接收数据：使用 channel 的 receive 操作可以从 channel 中接收数据，如果 channel 为空，receive 操作会阻塞当前协程直到 channel 中有数据。
3. 关闭 channel：使用 close 操作可以关闭 channel，通知所有协程该 channel 已经不再使用。关闭 channel 后，所有的 send 操作都会失败，而且**可以通过检查 channel 是否已经关闭来判断 channel 是否已经被取完**。



#### 怎么知道channel有没有被取完？【1】

**select 和 default**

例如，可以编写一个 for 循环，不断地从 channel 中读取数据，如果 channel 已经被取完，则会阻塞在读取操作上，此时可以利用 select 语句的 default 子句，从而在不阻塞的情况下执行一些其他的操作，例如结束循环或者打印日志等。

（以下是一个示例代码，演示了如何使用 channel 进行协程同步，并且如何使用 select 语句检测 channel 是否已经被取完：）

```go
package main

import (
    "fmt"
    "time"
)

func producer(ch chan<- int) {
    for i := 0; i < 10; i++ {
        ch <- i
        fmt.Printf("producer: %d\n", i)
        time.Sleep(100 * time.Millisecond)
    }
    close(ch)
}

func consumer(ch <-chan int) {
    for {
        select {
        case x, ok := <-ch:  // 如果 channel 还有数据可读，则会进入 case 子句，从 channel 中读取数据，并打印一条日志信息
            if !ok {  // 当 producer 协程向 channel 中发送完所有数据后，会通过 close 函数关闭 channel，从而 consumer 协程在检测到 channel 被关闭时结束循环，程序执行完毕。
                fmt.Println("channel is closed")
                return
            }
            fmt.Printf("consumer: %d\n", x)
            time.Sleep(200 * time.Millisecond)
        default:  // 如果 channel 已经被取完，则会进入 default 子句，打印一条提示信息，并在短暂的等待后继续执行循环
            fmt.Println("channel is not ready yet")
            time.Sleep(50 * time.Millisecond)
        }
    }
}

func main() {
    ch := make(chan int)
    go producer(ch)
    consumer(ch)
}

```

需要注意的是，如果 channel 中的数据量较大，或者生产者协程的速度远快于消费者协程，就需要使用带缓冲的 channel，或者使用多个消费者协程并发消费数据，以提高程序的吞吐量和性能。



**range 关键字遍历 channel**

如果 channel 已经被关闭并且其中的数据已经全部被取完，range 循环会自动退出。例如：

```go
ch := make(chan int, 10)
// 向 channel 中发送数据
for i := 0; i < 10; i++ {
    ch <- i
}
// 关闭 channel
close(ch)
// 遍历 channel，取出其中的数据
for v := range ch {
    fmt.Println(v)
}

```





#### 缓冲channel 和 无缓冲 channel

1. 无缓冲channel，创建方式为 make(chan TYPE) 。每一个发送者与接收者都会阻塞当前线程，只有当接受者与发送者都准备就绪了，channel才能正常使用。
2. 缓冲channel 即 buffer channel 创建方式为 make(chan TYPE, SIZE) c2<-1 则不会阻塞，因为缓冲大小是3，只有前三个值都还没被人拿走，这时候才会阻塞。



#### 协程之间怎么通信/一组协程完成后需要通知其他协程，可以怎么办？【3】

新建一个协程监听某个channel。

1. 如果知道具体需要同步的协程数目，创建一个带缓冲区的channel，每个协程运行完毕后向channel中写入，当取出指定数目的元素后即表明全部完成
2. 如果不知道具体的数目。先建立无缓冲的channel，然后通过 sync.WaitGroup，每个协程启动前 wg.add(1)，每个协程返回的时候，wg.done()。 新建一个协程用来监听 waitGroup，即 wg.wait() 然后阻塞住。等执行完毕后，再关闭channel，主协程得知关闭后即已完成





#### channel的工作原理是什么？【2】

`channel`有以下特点：

1. channel 可以在协程之间传递数据，因此可以用来进行协程之间的同步和通信。
2. channel 是线程安全的，可以在多个协程中并发读写。
3. channel 可以是有缓冲的或者无缓冲的，可以通过设置 channel 的缓冲区大小来控制 channel 的行为。
4. **channel 可以使用 range 关键字进行遍历，也可以使用 select 语句进行多路复用。**

使用`channel`的基本操作包括：

1. 创建一个`channel`：使用`make`函数创建，可以指定其容量和类型。
2. 发送数据到`channel`：使用`<-`运算符将数据发送到`channel`中。
3. 从`channel`接收数据：使用`<-`运算符从`channel`中接收数据。
4. 关闭`channel`，关闭后，所有的 send 操作都会失败，并且从 channel 中读取数据的协程会在 channel 中已经没有数据的情况下自动退出。





#### 向一个已经关闭的channel发送和接收数据会发生什么【1】

对一个已经关闭的channel取数据，如果里面有值会先取值，当值完后会取到该channel类型的零值。

向一个已近关闭的channel发送数据，会panic: send on closed channel

关闭一个已经关闭的channel，会panic: close of closed channel

（遍历一个没有关闭的channel 会报dead lock）



#### 如何对同时执行某一任务的协程数量进行限制？

1. 有缓存的channel，对于并发任务，包裹起来

   <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-03-20%2017.38.50.png" alt="截屏2023-03-20 17.38.50"  />

2. 使用指定数量的协程去访问任务

   ![截屏2023-03-20 17.39.41](https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-03-20%2017.39.41.png)





#### 限定数目协程并发案例

`main` 函数启动了3个 worker 协程来处理任务（只是启动，由于jobs通道没有数据，3个协程会阻塞），然后将5个任务分配给这些协程。在主线程中，通过 `select` 语句等待 worker 协程处理完成所有任务。`results` 通道用于接收 worker 协程处理任务后的结果，主线程通过 `<-results` 语句从通道中取出结果并进行处理。当所有任务都被处理完成后，主线程退出。

```go
package main

import (
    "fmt"
    "time"
)

func worker(id int, jobs <-chan int, results chan<- int) {
    for j := range jobs {
        fmt.Printf("worker %d started job %d\n", id, j)
        time.Sleep(time.Second) // 模拟处理任务需要的时间
        fmt.Printf("worker %d finished job %d\n", id, j)
        results <- j * 2
    }
}

func main() {
    const numJobs = 5
    jobs := make(chan int, numJobs)
    results := make(chan int, numJobs)

    // 启动3个 worker 协程
    for i := 1; i <= 3; i++ {
        go worker(i, jobs, results)
    }

    // 分配5个任务给 worker 协程处理
    for j := 1; j <= numJobs; j++ {
        jobs <- j
    }
    close(jobs) // 关闭 jobs 通道，表示所有任务已经分配完成

    // 通过 select 语句等待 worker 协程处理完成所有任务
    for a := 1; a <= numJobs; a++ {
        <-results
    }
}

```





### 通用

#### 函数传指针和传值有什么区别？【1】

值传递只会把参数的值复制⼀份放进临时生成的内存空间，两个变量的地址不同，不可相互修改

引⽤传递会将变量的内存地址寻找方式传入，在函数中可以对该变量值的内容进行修改

golang默认都是采用值传递（array也是值传递），即拷贝传递，有些值默认就是指针（slice、map、channel） 

interface是引用传递！！



#### defer【1】

`defer` 是 Go 语言中的一个关键字，用于在函数返回前执行一些操作，比如释放资源或打印日志等。`defer` 语句可以在函数体中的任何位置进行定义，但是它们的执行顺序与它们的定义顺序相反。

<font color=fuchsia>**defer语句中的函数会在return语句更新返回值变量后再执行**</font>



**情景1:**

在下面的例子中，变量 `a` 的值被拷贝到了函数的栈空间中，函数结束时，栈空间中的值将被弹出并被丢弃，而不会对原始变量 `a` 的值造成任何影响。

因此，在函数结束前，即使在 `defer` 语句中修改了变量 `a` 的值，也不会影响函数的返回值。



当 main 函数调用 foo 函数时，会在函数栈中分配一个名为 a 的局部变量，并将 a 的值初始化为 0。

然后在 foo 函数中，定义了一个 defer 函数。这个 defer 函数将在 foo 函数**返回前**被执行，defer 函数对变量 a 增加了 2，此时 a 的值变为 2。但是，这个 defer 函数只是在函数返回前执行完毕，尚未输出。

接着，在 foo 函数中，执行 return a + 1，将 a 的值 0 增加 1 后作为函数的返回值。由于 defer 函数是已经被调用的，因此 a 值已经被增加 2。

最后，main 函数接收到 foo 函数的返回值，输出了 1。

```go
func foo() int {
    var a int = 0
    defer func() {
        a += 2
        fmt.Println(a)  // 输出 2
    }()
    return a + 1
}

func main() {
    fmt.Println(foo()) // 输出 1
}

```



**情景2:命名返回值**

函数 `foo()` 使用了命名返回值 `a`。

当调用 foo 函数时，会在函数栈中分配一个名为 a 的局部变量。

接着，在函数中定义 defer 函数，即 defer 后的匿名函数。这个匿名函数会在 foo 函数返回时被执行。

然后，执行 return 1，将 1 赋值给返回值 a，并开始执行 defer 函数。执行 defer 函数时，修改了 a 的值（a = a + 2），此时 a 的值变为了 3。

在 defer 函数执行完成之后，foo 函数**正式返回**，将 a 的值作为返回值返回。由于 a 的值已经被 defer 修改过了，因此函数返回的值为 3。

因此，foo 函数最终的返回值是 3。



**命名返回值的执行顺序：return 值 -> 命名返回值 -> defer 中的变量 -> 产生修改 -> 命名返回值**

```go
func foo() (a int) {
	defer func() {
		a += 2
		fmt.Println(a)  // 输出 3
	}()
	return a + 1
}

func main() {
	fmt.Println(foo())  // 输出 3
}
```



#### 对象是什么。面向对象有什么好处？go 中如何实现多态

对象是类的实例，是面向对象编程中基本的运行实体。

面向对象的好处：

- 封装可以提高类的易用行、减少编程过程中代码出错的风险
- 继承可以实现代码的复用
- 抽象可以让程序的设计和实现分离
- 多态提高了程序的可拓展性

Go 中一个类型如果定义了接口的所有方法，那它就隐式地实现了该接口。

所有实现了接口的类型，都可以把它的值保存在一个接口类型的变量中。在 Go 中，我们使用接口的这种特性来实现多态。



#### go怎么实现封装、继承、多态【1】

**封装**

大小写



**继承**

如果一个struct嵌套了另一个匿名结构体，那么这个结构体可以直接访问匿名结构体的字段和方法，从而实现继承特性。 同时，一个struct还可以嵌套多个匿名结构体，那么该struct可以直接访问嵌套的匿名结构体的字段和方法，从而实现多重继承。



**多态**

（基类指针可以指向任何派生类的对象，并在运行时绑定最终调用的方法的过程被称为多态。）

golang中采用接口实现多态。golang里面有一个接口类型interface，任何类型只要实现了接口的方法，都可以赋值给该接口类型。



### go 如何实现并发安全？

- mutex
- channel
- 并发安全的数据结构
- wait group



### sync 和 atomic 库

**atomic**

`atomic`包提供了底层的原子级内存原语，用于在多线程环境下的数据同步。

- **用途**：当你想要对一个变量进行极小的同步操作（如自增、自减、交换等）时，可以使用`atomic`包。
- **方法**：例如`AddInt32`, `CompareAndSwapInt32`, `LoadInt64`等。
- **性能**：由于原子操作是硬件层面的支持，所以通常比mutex锁更快。
- **限制**：原子操作只适用于一些简单的场景，并且被限制于一些特定的类型。

```
var counter int32

func increment() {
    atomic.AddInt32(&counter, 1)
}

```



**sync**

`sync`包提供了更高级的同步原语，例如互斥锁（Mutex）和读写锁（RWMutex）、Once、WaitGroup







## 协程

#### 协程的概念【1】

在 Go 语言中，协程（goroutine）是一种轻量级的线程，由 Go 运行时管理。与传统的线程相比，协程的切换开销更小，因为它们不需要操作系统的干预。



#### 为什么Golang可以很容易实现高并发【1】

https://blog.csdn.net/big_white_py/article/details/111465167

1. Golang实现了 CSP 并发模型做为并发基础。Goroutine 是go实际并发执行的实体。底层使用协程（coroutine）实现并发，协程是一种运行在用户态的用户线程，避免了内核态和用户态的切换；可以由语言和框架层进行调度；具有更小的栈空间，允许创建大量的实例
2. **实体间通过 channel 进行匿名消息传递**，实现实体中间的解耦。
3. 在语言层面实现了自动调度，goroutine是可以被异步抢占。并且go对网络IO库进行了封装，屏蔽了很多内部细节，对外提供统一的语法关键字支持，简化了并发程序编写的成本。





#### 什么时候需要控制协程并发的数量？

（例如我有100条/100万条新闻，是应该有多少开多少协程，还是控制数量？）



**考虑控制并发数量的情况**：

- 资源限制：计算机的CPU、内存和其他资源是有限的。过多的并发可能导致资源竞争，反而降低整体性能。
- I/O限制：如果任务涉及网络、磁盘或数据库访问，过多的并发可能导致I/O瓶颈，甚至触发操作系统或服务提供者的限制。

- 错误处理：高并发可能导致更多的错误（如网络超时、服务不可用等）。控制并发可以更好地管理错误，并确保系统的鲁棒性。

- 代码复杂性：管理大量并发可能会增加代码复杂性，特别是如果有共享资源或依赖关系。




**开启多少协程或控制数量**：

- 对于100条新闻：如果任务相对轻量，例如内存中的数据处理，直接开启100个协程可能是合理的。
- 对于100万条新闻：直接开启100万个协程可能会导致资源消耗、I/O瓶颈或其他问题。在这种情况下，控制并发数量可能更合适。




**建议解决方案**：

- 使用协程池：协程池可以让你限制并发数量，同时充分利用资源。（本质上是生成固定数量的协程监听一个 job channel，发布任务时 wg.Add(1)，完成任务时 wg.Done()）

- 动态调整：你可以根据系统负载和任务特性动态调整并发数量。

- 测试和调优：通过基准测试和性能分析来找到适合你的具体情况和硬件配置的最佳并发级别。



#### 除了并发之外，还有什么能提高速度吗？【缓存】









### ⭐️GMP模型【4】

https://blog.csdn.net/qq_39679639/article/details/124372554

CPU线程切换的开销是影响性能的一个因素，Go提供了一种机制，可以在用户空间实现任务的切换，上下文切换成本更小，可以达到使用较少的线程数量实现较大并发的能力，即**GMP模型**

- `G(Goroutine)`: 表示 Go 语言中的协程，它是并发的执行单元
- `M(Machine)`: 代表着一组系统资源，主要包括一个操作系统线程、调度器以及堆栈等资源
- `P(Processor)`: 代表着处理器，用来执行 G。P 是 M 的执行上下文，它可以和 M 绑定或解绑，以便运行 G。通常情况下，每个线程都会绑定一个 P。

**`M`必须拥有`P`，才能执行`G`中的代码，`P`负责`G`的调度**，



<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f3030387678766747677931683863367065627a6f396a333079343075307461302e6a7067.jpeg" alt="截屏2022-11-21 03.06.01" style="zoom: 33%;" />

#### Processor 调度【4】

https://blog.csdn.net/weixin_36750623/article/details/127022527

**Processor常规调度**

<font color=grey>一个线程M想执行协程G：M就要先去「空闲P队列」获取P，然后P和M绑定，之后P再依次去「本地协程队列、全局协程队列」获取G，将G交给线程M去执行</font>

每个`P`维护一个`G`队列，`P`周期性的将`G`调度到`M`上执行一小段时间，然后保存上下文，并将次`G`放到队列末尾，继续执行下一个`G`



**work-stealing机制**

- 场景：当本线程⽆可运⾏的G时，尝试「从其他线程绑定的P偷取G」
- 获取的流程：
  - 从本地队列获取任务
  - 从全局队列获取任务
  - 从其它M的本地队列窃取任务



**hand-off机制（切换机制）**

- 场景：当本线程因为G进⾏系统调⽤阻塞时，线程M释放绑定的P，把P转移给其他空闲的线程执⾏
- 流程：当G阻塞时，与该G绑定的M也会陷入阻塞，在阻塞之前，会先把M绑定的P转移给其他M'（复用或新建），然后将CPU切换到M'去执行





⚠️**如何执行协程切换的？**【1】

https://zhuanlan.zhihu.com/p/29887309

TLS

thread local storage:

getg()

goget()用来获取当前线程正在执行的协程g。该协程g被存储在TLS中。

mcall()

mcall在golang需要进行协程切换时被调用，用来保存被切换出去协程的信息，并在当前线程的g0协程堆栈上执行新的函数。一般情况下，会在新函数中执行一次schedule()来挑选新的协程来运行。





### 程序练习

#### go 循环输出n个 0 和 1【2】

watingGroup写法

```go
package main

import (
	"fmt"
	"sync"
)

func main() {
	zeroChan := make(chan bool)
	oneChan := make(chan bool)

	wg := sync.WaitGroup{}

	wg.Add(1)
	go func() {
		defer wg.Done()
		for i := 0; i < 20; i++ {
			<-zeroChan
			fmt.Println(0)
			oneChan <- true
		}
	}()

	wg.Add(1)
	go func() {
		wg.Done()
		for i := 0; i < 20; i++ {
			<-oneChan
			fmt.Println(1)
			zeroChan <- true
		}
	}()

	// 启动第一个协程
	zeroChan <- true

	wg.Wait()
}

```



#### go多协程按顺序打印数字 0-9【1】

（2023.08 字节三面）

```go
func main() {
	wg := sync.WaitGroup{}

	chs := make([]chan int, 10)  // 注意这里channel切片的定义！ []chan int
	for i := 0; i < 10; i++ {
		chs[i] = make(chan int)
	}

	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func(i int) {
			defer func() {
				wg.Done()
			}()

			<-chs[i]
			fmt.Println(i)
			if i < 9 {
				chs[i+1] <- i + 1
			}
		}(i)
	}

	chs[0] <- 0
	wg.Wait()
}

```



## GC

发展：标记清理 -> 三色标记法 -> 混合写屏障

https://blog.csdn.net/qq_45738177/article/details/125359749

https://zhuanlan.zhihu.com/p/518984548

https://www.bilibili.com/video/BV1wz4y1y7Kd?p=6&vd_source=f86a3c07e572293f592691638d279e85

### 标记清除（Go V1.3）

- 暂停程序（Stop The World, STW），将程序中的对象分为可达和不可达
- 对可达对象进行标记，然后清除不可达对象
- 停止暂停，程序继续执行，直到程序结束



### 三色标记法（Go V1.5）

三色可达性分析标记算法按“是否被访问过”将程序中的对象分成白色、黑色和灰色：

- 白色对象 — 对象尚未被垃圾收集器访问过，在可达性分析刚开始的阶段，所有的对象都是白色的，若在分析结束阶段，仍然是白色的对象，即代表不可达。
- 黑色对象 — **表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经被扫描过**，黑色的对象代表已经被扫描过而且是安全存活的，如果有其他对象指向黑色对象无需再扫描一遍，黑色对象不可能直接（不经过灰色对象）指向某个白色对象。
- 灰色对象 — **表示对象已经被垃圾收集器访问过，但是这个对象上至少存在一个引用还没有被扫描过**，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象。



1. 存在STW的三色标记法

   - 程序起初创建的对象都设为白色
   - GC开始时从根结点开始遍历所有对象，把遍历（非递归遍历）到的对象从白色标记为灰色
   - 遍历灰色对象，将灰色对象引用的对象标记为灰色，同时将该灰色对象标记为黑色
   - 重复上一步骤，直到没有灰色标记的对象
   - 将剩余所有白色结点进行删除回收

2. 没有STW的三色标记法

   如果并发执行（不使用STW），用户程序可能在标记执行的过程中修改对象的指针。

   可能导致：

   - 把原本应该垃圾回收的死亡对象错误的标记为存活，不过会在下次垃圾收集中清理。
   - 把原本存活的对象错误的标记为已死亡，导致“对象消失”，这在内存管理中是非常严重的错误。

   <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838633035676c6c386c6a3330733030713864676b2e6a7067.jpeg" alt="截屏2022-11-20 23.19.14" style="zoom:50%;" />



### 屏障技术

“对象消失”的问题的两个条件：

- 赋值器插入了一条或多条从黑色对象到白色对象的新引用；
- 赋值器删除了全部从灰色对象到该白色对象的直接或间接引用。

保证垃圾收集算法的正确性，只需破坏这两个条件的任意一个即可，屏障技术就是在并发或者增量标记过程中保证三色不变性的重要技术。



**强三色不变式**

强制不允许黑色对象引用白色对象

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168386330636f75346f756a33306c793062796d78682e6a7067.jpeg" alt="截屏2022-11-20 23.26.13" style="zoom:50%;" />

**弱三色不变式**

黑色对象可以引用白色对象，白色对象存在其他灰色对象对它的引用，或者可达它的链路上游存在灰色对象。（即所有被黑色对象引用的白色对象都被灰色对象保护着）

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f30303876787667476779316838633066326a327a316a3331376d3069676a73752e6a7067.jpeg" alt="截屏2022-11-20 23.28.30" style="zoom:50%;" />



#### 插入屏障

`具体操作`：在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色)

`满足`：**强三色不变式**. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色)

注：**插入屏障只在堆区对象中使用**，在栈中执行三色标记过程会启动STW。在准备回收白色前，重新遍历扫描一次栈空间，加STW暂停保护栈，将栈的对象进行一次三色标记，直到没有灰色节点，再进行垃圾回收。

不足：结束后需要STW重新扫描栈。

```
func DijkstraWritePointer(slot *unsafe.Pointer, ptr unsafe.Pointer) 
     shade(ptr)  //先将新下游对象 ptr 标记为灰色
     *slot = ptr
}

//说明：
添加下游对象(当前下游对象slot, 新下游对象ptr) { 
 //step 1
 标记灰色(新下游对象ptr) 
 
 //step 2
 当前下游对象slot = 新下游对象ptr 
}

//场景：
A.添加下游对象(nil, B) //A 之前没有下游， 新添加一个下游对象B， B被标记为灰色
A.添加下游对象(C, B) //A 将下游对象C 更换为B， B被标记为灰色
```





#### 删除屏障

`具体操作`: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。

`满足`: **弱三色不变式**. (保护灰色对象到白色对象的路径不会断)

不足：一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉。回收精度较低。

```
func YuasaWritePointer(slot *unsafe.Pointer, ptr unsafe.Pointer) {
    shade(*slot) 先将*slot标记为灰色
    *slot = ptr
}
//说明：
添加下游对象(当前下游对象slot， 新下游对象ptr) {
  //step 1
  if (当前下游对象slot是灰色 || 当前下游对象slot是白色) {
          标记灰色(当前下游对象slot)     //slot为被删除对象， 标记为灰色
  }  
  //step 2
  当前下游对象slot = 新下游对象ptr
}

//场景
A.添加下游对象(B, nil)   //A对象，删除B对象的引用。B被A删除，被标记为灰(如果B之前为白)
A.添加下游对象(B, C)     //A对象，更换下游B变成C。B被A删除，被标记为灰(如果B之前为白)
```



### 三色标记法+混合写屏障（Go V1.8）

（建议看完下面这个视频的后几集）

https://www.bilibili.com/video/BV1wz4y1y7Kd?p=9&vd_source=f86a3c07e572293f592691638d279e85

https://blog.csdn.net/weixin_43566459/article/details/123907548

`具体操作`：

- GC开始时将栈上的对象**全部扫描**并标记为黑色(之后不再进行第二次重复扫描，无需STW)，
- GC期间，任何在栈上<font color=blue>创建</font>的新对象，均为黑色。
- 堆中被删除的对象标记为灰色，然后将其从内存中删除，并将其关联的所有对象标记为灰色，表示这些对象需要被进一步扫描。
- 堆中被添加的对象标记为灰色。

注：栈不触发屏障机制！即栈中被删除/添加的对象不会标记为灰色。（第二行“任何在栈上<font color=blue>创建</font>的新对象，均为黑色”是有效的，因为这不是屏障机制）



应用场景：

1. 对象被一个堆对象删除引用，成为栈对象的下游
2. 对象被一个栈对象删除引用，成为另一个栈对象的下游
3. 对象被一个堆对象删除引用，成为另一个堆对象的下游
4. 对象从一个栈对象删除引用，成为另一个堆对象的下游











#### 三色标记法的灰色、黑色有什么区别？为什么区分灰色和黑色，灰色存在的意义？【1】

- 黑色对象 — **表示对象已经被垃圾收集器访问过，且这个对象的所有引用都已经被扫描过**，黑色的对象代表已经被扫描过而且是安全存活的，如果有其他对象指向黑色对象无需再扫描一遍
- 灰色对象 — **表示对象已经被垃圾收集器访问过，但是这个对象上至少存在一个引用还没有被扫描过**，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象。

灰色存在的意义在于，它可以表示当前正在被处理的对象，从而避免重复扫描和处理。



#### 写屏障是什么？【1】

内存屏障技术是一种指令，它可以让CPU或者编译器在执行内存相关操作时**遵循特定的约束**，像是一个钩子函数。

【插入屏障和删除屏障的相关说明】



#### 什么是内存逃逸，在什么情况下发生，原理是什么?【1】

https://www.11meigui.com/2022/golang-memory-escape.html

栈是线程级别的，大小在创建的时候已经确定，当变量太大的时候，会"逃逸"到堆上，这种现象称为**内存逃逸**。



内存逃逸的场景：

- 指针逃逸：

  Go可以返回局部变量指针，这种情况下，函数虽然退出了，但是因为指针的存在，对象的内存不能随着函数结束而回收，因此只能分配在堆上。

- 栈空间不足逃逸：

  当栈空间不足以存放当前对象时或无法判断当前切片长度时会将对象分配到堆中

- interface{} 动态类型逃逸

  在 Go 中，空接口 interface{} 可以表示任意的类型，如果函数参数为 interface{}，编译期间很难确定其参数的具体类型，也会发生逃逸。

- 闭包引用对象逃逸



**逃逸分析基本原则**

编译器会根据变量是否被外部引用来决定是否逃逸：

- 如果函数外部没有引用，则优先放到栈中；
- 如果函数外部存在引用，则必定放到堆中;
- 如果栈上放不开，则必定放到堆上;



**内存逃逸分析的好处**

- 栈上分配内存比在堆中分配内存效率更高
- 栈上分配的内存不需要 GC 处理，而堆需要
- 逃逸分析目的是决定内分配地址是栈还是堆
- 逃逸分析在编译阶段完成



### （扩）Java GC

1. **对象的创建**：JVM会在堆内存中为其分配空间。这些对象通过Java程序中的引用变量进行访问。
2. **对象的使用**：只要对象被引用，它就会一直存在。当对象的引用被赋值为null，或者引用变量超出了其作用域，那么这个对象就不再被使用。
3. **垃圾回收**：引用计数和可达性分析。引用计数是通过计算对象的引用数量来判断对象是否可以被回收，但这种方法无法解决循环引用的问题。Java的垃圾回收器主要使用可达性分析算法，它从一组根对象（如全局变量、局部变量、线程等）开始，通过引用链来判断对象是否可达，如果一个对象不可达，那么这个对象就可以被回收。
4. **垃圾回收器**：Java有多种垃圾回收器，如Serial GC、Parallel GC、CMS（Concurrent Mark Sweep）GC、G1（Garbage-First）GC等，它们使用不同的算法和策略来进行垃圾回收，以满足不同的性能需求。
5. **内存分区**：Java的堆内存被分为新生代（Young Generation）和老年代（Old Generation）。新创建的对象首先被放在新生代，经过多次垃圾回收后仍然存活的对象会被移动到老年代。新生代又被分为Eden区和两个Survivor区。这种内存分区策略有助于提高垃圾回收的效率。



**关于内存分区：**

Java的堆内存主要被分为两个部分：新生代（Young Generation）和老年代（Old Generation）。这种内存分区策略是基于这样一个观察：大部分对象的生命周期都很短，即“弱代假说”。

1. **新生代（Young Generation）**：新创建的对象首先被分配到新生代。新生代又被分为三个部分：一个Eden区和两个Survivor区（Survivor 0和Survivor 1）。大部分情况下，新创建的对象都被分配在Eden区。当Eden区满时，会触发一次Minor GC（也称为Young GC），在这次垃圾回收中，存活的对象会被移动到一个Survivor区。为了更有效地管理内存，Survivor区使用了复制算法，即每次Minor GC后，存活的对象会在两个Survivor区之间复制来复制去。当一个对象在Survivor区中存活了足够长的时间（或者Survivor区满了）后，它会被晋升到老年代。
2. **老年代（Old Generation）**：老年代用于存放长期存活的对象和大对象。当新生代中的对象经过一定次数的GC后仍然存活，或者对象太大无法在新生代中分配时，这些对象会被移动到老年代。当老年代满时，会触发一次Major GC（也称为Full GC或Old GC），这次垃圾回收会清理整个堆内存，包括新生代和老年代。Major GC通常比Minor GC要慢，因此应尽量避免触发Major GC。
3. **永久代（PermGen）或元空间（Metaspace）**：这部分内存主要用于存放JVM加载的类信息、常量、静态变量等数据。在Java 8之前，这部分内存被称为永久代，它是堆内存的一部分，有固定的大小。从Java 8开始，永久代被移除，改为使用元空间，元空间不在Java堆中，而是使用本地内存，因此其大小只受本地内存限制。



# C++

#### 深拷贝

深拷贝通常涉及创建一个新对象，然后将现有对象的内容复制到新对象中。这不仅包括复制对象的成员变量的值，还包括复制指针成员变量所指向的对象。

例如，如果你有一个类，它包含一个指向其他对象的指针，你可能需要在拷贝构造函数中创建一个新的对象，并将现有对象的内容复制到新对象中。

深拷贝通常需要自定义拷贝构造函数和赋值运算符重载函数。具体的实现取决于你的类的结构和你的需求。





# 中间件





## 消息队列

消息队列（Message Queue）则是一种异步通信机制，它将消息发送者和接收者解耦，通过消息队列传递消息。

消息队列通常包括一个消息队列服务器和多个客户端，客户端可以发布消息到队列，也可以订阅特定的队列以接收消息。消息队列可以提高系统的可伸缩性、弹性和可靠性，因为它能够让系统各个部分进行异步通信，避免了同步调用可能出现的问题，例如阻塞和死锁。

在实现上，消息队列中，数据传递是通过将消息写入队列，然后由另一个进程读取队列来完成的。





**概念**

消息队列是应用之间异步通信的方式，主要由三个部分组成：

生产者：生产消息的一端，主要负责消息所承载业务信息的一个实例化，是整个消息发起方

Broker（代理）：是消息的服务端，主要处理消息单元，负责消息的存储、投递等功能，是整个消息队列最核心的部分

消费者：消息的接收者，根据拿到的消息去处理各种业务逻辑。







**场景**

1. 异步处理

   <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-27%2019.21.29.png" alt="截屏2023-05-27 19.21.29" style="zoom:50%;" />

   实时性要求不严格的一些场景，（比如用户注册发送验证码）。

   服务方只需要把协商好的消息发送到消息队列里，剩下的由消费者的消息服务去处理，不需要等待消费者的返回结果就可以直接返回

2. 应用解耦

   <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-27%2019.23.30.png" alt="截屏2023-05-27 19.23.30" style="zoom:70%;" />

   把一些相关的但是耦合度不高的系统关联起来，（例如订单系统与优惠券、积分系统），每个系统之间只需要把一些约定的消息发送到MQ，另一个系统直接消费即可，可以解决各类采用不同的框架/语言实现系统之间的消息传递，增加系统灵活度。

3. 流量缓冲

   <img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/%E6%88%AA%E5%B1%8F2023-05-27%2019.25.58.png" alt="截屏2023-05-27 19.25.58" style="zoom:70%;" />

   在大流量入口的一些短时间业务，权衡高可用把大量的并行任务发送给MQ，根据MQ和存储和分发的功能平稳处理后续的业务，起到大流量缓冲的作用



### ⚠️Kafka









# Web

## 通用

### RESTful

1. URL应仅包含资源（名词）而不包含动作或者动词，HTTP方法(GET,POST,DELETE,PUT)作为动词

   **GET 方法**从资源请求数据，不应产生任何其他作用

   **POST方法**请求服务器在数据库中创建资源，主要是在提交Web表单时

   **PUT方法**请求服务器更新资源或创建资源（如果不存在）（注：PUT是全量更新！即更新项只有一项，也会把其他已有的设置为空）

   **DELETE方法**请求从数据库中删除资源或其实例

2. 使用JSON作为通信格式

   当客户端通过API向服务器发出请求时，客户端应该知道反馈

3. 使用HTTP状态码

4. 搜索，排序，过滤和分页

5. 使用版本控制

6. 路径中单词连接使用中划线-





## ⚠️微服务



有了解过微服务的基本原理吗？



微服务是怎么实现鉴别不同的服务的？









## 负载均衡

#### 💧一致性

raft

https://zhuanlan.zhihu.com/p/404786050



一致性哈希

https://zhuanlan.zhihu.com/p/482549860?utm_medium=social&utm_oi=919687111576289280





#### nginx负载均衡算法

1. 轮询（默认）
   每个请求按照时间顺序逐一分配到下游的服务节点，如果其中某一节点故障，nginx 会自动剔除故障系统使用户使用不受影响。
2. 权重（weight）
   在配置文件中对下游的服务节点指定权重值 weight, weight 值越大则被分配的评率越高，一般这种负载均衡，用于节点的配置情况不一样，有的可能配置高，有的配置低。
3. **ip_hash**
   对每个请求，针对 ip 进行 hash， 然后分配到后台节点，这样一来，同一 ip 会被固定分配到下游固定服务上。它能够暂时的解决集群环境中容器之间 session 共享的问题，但是不是解决的根本之道，只是权宜之策，我们试想，如果访问的好好的，家里的路由器被重启了，或者运营商分配给你的 ip 地址改变了，那么你再次访问的时候，新的 ip 就可能被分配到新的服务上，之前的 session 也就失效了。
4. least_conn (最少连接调度算法)
   最少连接调度算法，对下游服务中连接情况，优先选择连接数最少的服务分配。
5. least_time (最小响应时间)
   最小响应时间, 计算节点平均响应时间，然后取响应最快的那个，分配更高权重。



**配置案例**

轮询

```
upstream webserver {
	server 192.168.214.133:80 max_fails=3 fail_timeout=30s;
	server 192.168.214.187:80 max_fails=3 fail_timeout=30s;
}
```



加权轮询

```
upstream webserver {
	server 192.168.214.133:80 weight=3 max_fails=3 fail_timeout=30s;
	server 192.168.214.187:80 max_fails=3 fail_timeout=30s;
}
```



ip_hash

```
upstream webserver {
	ip_hash;
	server 192.168.214.133:80 max_fails=3 fail_timeout=30s;
	server 192.168.214.187:80 max_fails=3 fail_timeout=30s;
}
```



least_conn

```
upstream webserver {
	least_conn;
	server 192.168.214.133:80 weight=3 max_fails=3 fail_timeout=30s;
	server 192.168.214.187:80 max_fails=3 fail_timeout=30s;
}
```





### ⚠️zookeeper



## cookie, session and JWT

### Cookie

#### 属性

**Expires，Max-Age**

Expires：具体的到期时间。过期后，浏览器就不再保留这个cookie。

Max-Age：从现在开始cookie存在的秒数，优先级高于Expires。

不设置/null：cookie只在当前会话存在，浏览器窗口关闭后，当前session结束，cookie就被删除。



**Domain，Path**

（1）Domain：**指定cookie属于哪个域名**。以后浏览器向服务器发送请求时，通过该属性判断是否要携带cookie。默认为当前域名。

（2）Domian属性的规则：Domain属性只能是**当前域名或当前域名的上级域名**，且**不能是顶级域名或公共域名**，否则浏览器会拒绝设置该cookie。----浏览器发送cookie时，Domain属性必须是当前域名或其上级域名。

（3）Path：指定**浏览器发送http请求时，哪些路径要附带这个cookie**。只要Path属性是**请求路径的开头一部分**，就会在头信息中带上这个cookie。（前提是Domain符合条件）



**Secure，HttpOnly**

（1）Secure：指定浏览器只有在https协议下（当设置为true时），才能将cookie发送到服务器。如果当前协议是http，则会忽略secure属性。

（2）HttpOnly：指定**该cookie无法通过JS脚本拿到**，例如document.cookie、XMLHttpRequest对象都拿不到该属性，**只有浏览器发送http请求时才会带上该cookie**。可以防止[xss](https://so.csdn.net/so/search?q=xss&spm=1001.2101.3001.7020)攻击，不让cookie被脚本读到。



**SameSite**

（1）用来**防止用户追踪和防止csrf攻击**。

（2）**第三方cookie**：由第三方网站引导而附带发送的cookie。它不仅会导致csrf攻击，还能用于用户追踪（facebook网站在第三方网站插入一张图片，当浏览器执行该部分代码时，就会向facebook发送带cookie的请求，从而让facebook知道了你是谁，访问了什么网站）。

（3）可以设置的3个值，来限制第三方cookie。

- Strict：禁止第三方cookie。跨站点时，任何情况下都不会发送cookie。
- Lax：大多情况下也是禁止第三方cookie，但是***\*导航到目标网站的get请求（包含链接、get表单等）\****除外。默认值
- None：关闭SameSite属性，前提是同时设置secure属性，否则无效。



#### 安全

**csrf攻击**

csrf攻击---跨站请求伪造：**恶意网站伪造了带有正确cookie的http请求。**

举例：用户登录微博www.weibo.com后，微博服务器会发送一个cookie，用户后来又访问了恶意网站，网站中有www.weibo.com/api（子作用域），那么当用户访问该连接时，微博服务器就会收到带有正确cookie的请求，服务器就会认为是本人在操作，从而导致账号被劫持。



防御措施：

- 设置SameSite：Strict或Lax都可以，让cookie不随第三方网站发出。
- 验证token：浏览器发起请求后，服务器会返回一个token。之后每次请求都要同时带上token和cookie才会被认为是合法请求。



**xss攻击**

xss攻击——跨站脚本攻击：**注入恶意script代码到网页中，当用户浏览该页面时，就会执行这些script代码，从而实现恶意攻击。**

常见的有3种类型：

（1）**存储型（持久）**：恶意代码***\*存储在目标服务器上\****，当浏览器发送请求时，该恶意代码会随响应报文传回并解析执行。【与反射型的区别在于提交代码是否会存储在服务端】

 场景：用户评论区，提交包含一条恶意代码的评论到数据库，其他用户看到该评论时，恶意代码会随响应报文返回，浏览器解析xss代码。

（2）**反射型（非持久）**：***\*恶意代码出现在url\****中，当发请求时会作为输入提交到服务端，然后该恶意代码随响应报文一起传给浏览器，最后浏览器会解析执行该恶意代码。

（3）**DOM型**：不经过服务端，通过修改dom节点，把恶意代码插入到了页面。【与后两者的区别在于不经过服务端】

DOM型属于JS自身的安全漏洞，前两种属于服务端的安全漏洞。



防御措施：

- HttpOnly：许多xss攻击目标是为了窃取用户cookie，设置httpOnly可以防止JS获取cookie。
- 输入检查：对输入内容的`<script>`等标签进行转义或过滤，转义<>&等字符。



### Session

**过程简介**

```
1. 用户输入其登录信息
2. 服务器验证信息是否正确，并创建一个session，然后将其存储在数据库中
3. 服务器为用户生成一个sessionId，将具有sesssionId的Cookie将放置在用户浏览器中
4. 在后续请求中，会根据数据库验证sessionID，如果有效，则接受请求
5. 一旦用户注销应用程序，会话将在客户端和服务器端都被销毁
```



### JWT

https://blog.csdn.net/abcnull/article/details/122871443

| 构成      | 名称          | 描述                                                         |
| --------- | ------------- | ------------------------------------------------------------ |
| Header    | 头部          | Token类型和签名算法（HMAC SHA256、HS384）。                  |
| Payload   | 载荷（Claim） | 载荷可以用来放一些不敏感的信息。<br />JWT 的签发者、接收者、面向用户、过期时间、签发时间 |
| Signature | 签名          | 由Header、Payload、自己维护的一个Secret经过加密得来。        |

头部和载荷分别进行Base64编码之后得到两个字符串，然后再将这两个编码后的字符串用英文句号`.`连接在一起，形成新的字符串。

将拼接完的字符串用HS256算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。加密后的内容也是一个字符串，最后这个字符串就是签名，把这个签名拼接在刚才的字符串后面就能得到完整的jwt。

header部分和payload部分如果被篡改，由于篡改者不知道密钥是什么，也无法生成新的signature部分，服务端也就无法通过，在jwt中，消息体是透明的，使用签名可以保证消息不被篡改。





**过程简介**

```
1. 用户输入其登录信息
2. 服务器验证信息是否正确，并返回已签名的token
3. token储在客户端，例如存在local storage或cookie中
4. 之后的HTTP请求都将token添加到请求头里
5. 服务器中间件解码JWT，并且如果令牌有效，则接受请求
6. 一旦用户注销，令牌将在客户端被销毁，不需要与服务器进行交互一个关键是，令牌是无状态的。后端服务器不需要保存令牌或当前session的记录。
```



**区别和优缺点**

基于session和基于jwt的方式的主要区别就是用户的状态保存的位置，session是保存在服务端的，而jwt是保存在客户端的。

 

**jwt的优点**

1. 可扩展性好

   应用程序分布式部署的情况下，session需要做多机数据共享，通常可以存在数据库或者redis里面。而jwt不需要。

2. 无状态

   jwt不在服务端存储任何状态。RESTful API的原则之一是无状态，发出请求时，总会返回带有参数的响应，不会产生附加影响。用户的认证状态引入这种附加影响，这破坏了这一原则。另外jwt的载荷中可以存储一些常用信息，用于交换信息，有效地使用 JWT，可以降低服务器查询数据库的次数。



 **jwt的缺点**

1. 安全性

   由于jwt的payload是使用base64编码的，并没有加密，因此jwt中不能存储敏感数据。而session的信息是存在服务端的，相对来说更安全。

2. 性能

   jwt太长。由于是无状态使用JWT，所有的数据都被放到JWT里，如果还要进行一些数据交换，那载荷会更大，经过编码之后导致jwt非常长，cookie的限制大小一般是4k，cookie很可能放不下，所以jwt一般放在local storage里面。并且用户在系统中的每一次http请求都会把jwt携带在Header里面，http请求的Header可能比Body还要大。而sessionId只是很短的一个字符串，因此使用jwt的http请求比使用session的开销大得多。

3. 一次性

   无状态是jwt的特点，但也导致了这个问题，jwt是一次性的。想修改里面的内容，就必须签发一个新的jwt。

   - 无法废弃

     通过上面jwt的验证机制可以看出来，**一旦签发一个jwt，在到期之前就会始终有效，无法中途废弃。**当信息需要更新时，则重新签发一个jwt，但是由于旧的jwt还没过期，拿着这个旧的jwt依旧可以登录，那登录后服务端从jwt中拿到的信息就是过时的。为了解决这个问题，我们就需要在服务端部署额外的逻辑，**例如设置一个黑名单，一旦签发了新的jwt，那么旧的就加入黑名单（比如存到redis里面）**，避免被再次使用。

   - 续签

     如果你使用jwt做会话管理，传统的cookie续签方案一般都是框架自带的，session有效期30分钟，30分钟内如果有访问，有效期被刷新至30分钟。一样的道理，要改变jwt的有效时间，就要签发新的jwt。最简单的一种方式是每次请求刷新jwt，即每个http请求都返回一个新的jwt。这个方法不仅暴力不优雅，而且每次请求都要做jwt的加密解密，会带来性能问题。另一种方法是在redis中单独为每个jwt设置过期时间，每次访问时刷新jwt的过期时间。

     

   **可以看出想要破解jwt一次性的特性，就需要在服务端存储jwt的状态。但是引入 redis 之后，就把无状态的jwt硬生生变成了有状态了，违背了jwt的初衷**。而且这个方案和session都差不多了。



**如何保证JWT的安全性？**

使用 HTTPS：JWT 会在客户端和服务端之间通过 HTTP 协议传输

JWT 中存储有限的信息





## 框架

#### 介绍Gin框架

**Gin 框架的优点**

【路由、中间件、崩溃恢复、json、渲染方式】

- 快速：基于Radix树的路由
- 路由分组：支持路由分组(RouteGroup)，可以更方便组织路由
- 支持中间件：内置许多中间件，如Logger,Gzip,Authorization, Recover等。
- 扩展性：非常简单扩展中间件。
- 崩溃恢复：可以捕捉panic引发的程序崩溃，使Web服务可以一直运行。
- JSON验证：可以验证请求中JSON数据格式。
- 多种数据渲染方式：支持HTML、JSON、YAML、XML等数据格式的响应。



#### 为什么使用Gin框架？Gin框架的具体实现？【1】

https://blog.csdn.net/qq_40369829/article/details/121645618



- 初始化

  - 创建Engine

    r := gin.New()生成了一个Engine对象，Engine对象是整个框架的核心，也包含了对路由的操作和许多成员变量，其中包括路由要执行的任务链HandlersChain，方法树methodTrees等。

  - 注册路由和中间件

    1. 计算绝对路径
    2. 添加handlers
    3. 关联绝对路径和handlers

  - 开启服务

    底层原生net/http

- 处理请求

  - 匹配路由
    1. 匹配HTTP method对应的基数树
    2. 基数树中寻找handlers
  - 执行
    1. 执行中间件+业务逻辑的职责链
    2. 如果自定义中间件，可以使用next()实现切面





### 中间件

Gin框架允许开发者在处理请求的过程中，加入用户自己的钩子（Hook）函数。这个钩子函数就叫中间件，中间件适合处理一些公共的业务逻辑，比如登录认证、权限校验、数据分页、记录日志、耗时统计等。



Gin中的中间件实际上还是一个Gin中的 gin.HandlerFunc。中间都是需要注册后才能启用的。

中间件分类：

- 全局中间件：全局中间件设置之后对全局的路由都起作用。
- 路由组中间件：路由组中间件仅对该路由组下面的路由起作用。
- 单个路由中间件：单个路由中间件仅对一个路由起作用。



#### Gin框架的中间件是怎么实现的？有了解吗？【1】

https://blog.csdn.net/weixin_38753143/article/details/125916902

https://zhuanlan.zhihu.com/p/507085115

1. 注册过程：Gin把中间件和处理函数统一定义为一个handleFunc，将这些 handleFunc 结合到一起组成一条处理函数链条`HandlersChain`，本质上就是一个由`HandlerFunc`组成的切片
2. 执行过程：通过索引遍历`HandlersChain`链条，中间使用c.next、c.Abort等函数来进行流程控制，c.Set() 和 c.Get() 这两个方法多用于在多个函数之间通过 c 传递数据的。





### 路由

路由的过程：

**注册路由**

r.GET("/user/:name", routeUser)定义一个GET请求，模糊匹配/user/:name。

1. 将相对路径 join为绝对路径
2. 【注册中间件在这一步？】判断handlersChain的长度，不能超过math.MaxInt8 / 2并且把路由方法装载到handlersChain里面去。
3. 关联绝对路径和handlers，存入engine.trees 路由树。
4. 返回当前对象，达到能使用链式操作的目的



**访问路由**

输入localhost:8080/user/abc，首先进入net的ServeHTTP()方法。然后被gin框架handleHTTPRequest()方法接受。遍历之前注册的路由树engine.tress。

使用算法匹配。





#### Gin框架的路由树的实现。【1】

https://zhuanlan.zhihu.com/p/491337692

https://blog.csdn.net/www_xuhss_com/article/details/123321209

https://www.cnblogs.com/yuanwebpage/p/16812610.html

使用 Radix Tree 基数树或压缩前缀树。是一种更节省空间的 Trie 树。

字典树是一棵多叉树。从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。除根节点外，每一个节点只包含一个字符。每个节点的所有子节点包含的字符都不相同。

基数树是对字典树的压缩。父节点下第一级子节点数小于 2 的都可以进行压缩，把子节点合并到父节点上

压缩字典树每个节点上存储着一个或多个字符；**父节点上存储的字符为所有子节点的公共前缀。**

<img src="https://cdn.jsdelivr.net/gh/Lenvia/md-pic@master/uPic/68747470733a2f2f747661312e73696e61696d672e636e2f6c617267652f303038767876674767793168386334617878396b356a3330717930666d74397a2e6a7067.jpeg" alt="截屏2022-11-21 01.42.56" style="zoom:50%;" />

对于有大量公共前缀的多个字符串，采用radix tree可以显著减少节点数量，加快匹配速度。而URL路由就存在大量的公共前缀，因此很适合用radix tree来存储路由信息。



httprouter中的基数树

```
type node struct {
	path      string    // 节点对应的字符串路径
	wildChild bool      // 是否为参数节点，如果是参数节点，那么 wildChild=true
    nType     nodeType  // 节点类型，有几个枚举值可以看下面nodeType的定义
	maxParams uint8     // 节点路径最大参数个数
	priority  uint32    // 节点权重，子节点的handler总数
	indices   string    // 节点与子节点的分裂的第一个字符
	children  []*node   // 子节点
	handle    Handle    // http请求处理方法
}

```



**httprouter概述**

gin的路由注册采用的是httprouter。httprouter是基于radix tree实现的前缀匹配。httprouter在实现radix tree时增加了一些针对url的优化：

（1）**每一种http方法各一个 radix tree**，这样可以提高匹配效率；
（2）提供了通配符 ":" 和 "\*"，其中 ":"+参数名称 表示路径参数，比如 GET /template/:user_id，其中 ":user_id" 就是一个路径参数，访问时用户输入url： /template/12345，则user_id=12345。"\*"+参数名称 为匹配所有参数，结构只能为 .../.../.../\*param_name，"\*" 只能作为结尾参数。





# 智力题

全都在下面的链接：

https://blog.csdn.net/qq_46588810/article/details/122088043

https://blog.csdn.net/qq_29966203/article/details/124213450



**1.赛马找最快<腾讯高频>**

**2.砝码称轻重**

**3.药瓶毒白鼠<腾讯>**

**4.绳子两头烧**

**5.犯人猜颜色**

**6.猴子搬香蕉**

**7.高楼扔鸡蛋<谷歌>**

**8.轮流取石子<字节>**

**9.蚂蚁走树枝**

**10.海盗分金币<不常见>**

**11.三个火枪手**

**12.囚犯拿豆子**

**13.学生猜生日<笔试高频>**





# 场景题

**设计一个邀请码，数字字母组成，固定长度，唯一，邀请码之间变化比较大。base64编码规则。**



**如果要设计一个秒杀系统，需要加锁？加到哪一层？代码？数据库？网关**

```
库存数量：在秒杀系统中，库存数量是最关键的数据之一。需要在减少库存数量的代码块中添加锁机制，避免多个线程同时减少库存数量导致数据不一致的问题。
用户请求：在秒杀系统中，用户请求是一个瓶颈，需要控制用户的请求速度和并发数量，避免服务器负载过高。可以通过添加限流措施（如令牌桶算法、漏桶算法等）控制用户请求的速度和数量。同时，也可以在用户请求的代码块中添加锁机制，避免多个线程同时处理同一个用户请求导致数据不一致的问题。
订单生成：在秒杀系统中，订单生成是最后的环节，也是最容易出问题的地方。需要在生成订单的代码块中添加锁机制，避免多个线程同时生成订单导致订单重复的问题。
加锁的具体实现方式有很多种，可以使用数据库锁、分布式锁、缓存锁等。一般来说，因为分布式锁和缓存锁的性能更好，更适合用于高并发的秒杀系统中。
```



**一个文件里有一百亿条数据，该怎么处理和保存，使得能够快速找到想要的数据？**

```
哈希到不同的桶里，对于每个桶，可以进行排序。

如果每个桶里的数据还是很大，那就分治，例如分成10块，对每一块分别排序。

寻找阶段：哈希值得到桶 -> 对于每个块 顺序/二分查找（当前块没有就继续下一个块）
```



**两个大文件找重复行**

```
具体步骤如下：
遍历第一个文件，对每一行进行哈希运算，将哈希值作为键，将行号（或行内容）作为值存入哈希表中。
遍历第二个文件，对每一行进行哈希运算，查找哈希表中是否已存在该哈希值，如果存在，则说明这一行与第一个文件中的某一行重复，记录下重复的行号（或行内容）。
当遍历完第二个文件后，所有重复的行号（或行内容）都已经记录下来了。
需要注意的是，由于文件较大，不能一次性将整个文件读入内存中处理，可以采用分块读取的方式，将文件分成若干个块，每次读取一个块进行处理，以减小内存的使用量。
此外，为了提高查找速度，可以使用一些哈希算法和哈希表实现方案，如Rabin-Karp哈希算法和布隆过滤器等，以提高查找的效率和减小哈希表的空间占用。
```



**维护热搜？**

https://www.zhihu.com/question/57968453







视频编码解码

工程处理视频的工具

AI可以在视频做什么
