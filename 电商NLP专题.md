## 八股

### bert 为什么scale product

当 $d_k$ 较大时，向量内积容易取很大的值。两个向量的内积均值为 0，而方差为$d_k$。

**方差较大，不同的 key 与同一个 query 算出的对齐分数可能会相差很大，有的远大于 0，有的则远小于 0.**

**很有可能存在某个 key，其与 query 计算出来的对齐分数远大于其他的 key 与该 query 算出的对齐分数。**

softmax函数对各个内积的偏导数都趋近于0，造成参数更新困难。

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gz7dvmrra6j30n403mq2y.jpg" alt="截屏2022-02-09 17.23.20" style="zoom:50%;" />



### transformer里encoder的什么部分输入给decoder

最后一个 Encoder block 输出的矩阵就是**编码信息矩阵**，这一矩阵后续会用到 Decoder 中。

<img src="https://tva1.sinaimg.cn/large/e6c9d24ely1gzrc1kk00aj20hs0p0gmo.jpg" alt="img" style="zoom:40%;" />

解码器的第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。





### MLM 为什么mask一部分保留一部分

Bert想要构造双向模型，利用了transformer的encoder。

为了让bert是双向的，提出mask LM避免深层的双向网络模型泄漏target。

Bert是深层，多层的网络结构会暴露预测词，失去学习的意义。



###  albert，roberta， electra做了什么改进

#### RoBERTa

RoBERTa 修改了预训练的方法，使用**动态掩码**、**舍弃NSP任务**。使用了更大的数据集。

**动态掩码：**

原始Bert中，训练集中的MASK是预先设置好的。在roberta中，**MASK的位置是在模型训练时实时计算决定的**。这样无论训练多少个epoch，都能极大程度避免数据重复。

**舍弃NSP任务：**

Bert的预训练任务2，使用句子对作为输入来判断是否相邻。但是roberta实验指出，去掉这个NSP任务效果会更好。

有一个解释是，Bert在选择NSP样本时，正样本是同一篇文章的连续段落，负样本是不同文章的两个段落。这样模型不仅关注句子的连通性，还会关注文章主题。而不同文章的主题相差很大，模型更容易倾向于主题完成任务。



### ALBERT

ALBERT认为Bert的参数量太大。提出了 词向量因式分解、跨层参数共享、句子顺序预测任务（SOP）。

**词向量因式分解：**

Bert中输入层的token的embedding层与transformer的隐藏层直接相连，这就导致embedding size必须等于hidden size。

这样的话映射矩阵就是一个V*H大小的矩阵。V是词表大小，H是隐藏层大小。这个矩阵非常大。

ALBERT认为，Bert主要是因为靠**隐藏层能够学到上下文之间的关系**，而且**本身词向量矩阵就是比较稀疏的**（因为输入的是one-hot，只有对应词索引的部分才会被激活），所以embedding的维度并不用和hidden层一样大。也就是说 H>>E。

因此，ALBERT通过一个V\*E的矩阵将词表压缩到较小的embedding维度，再通过一个E\*H的矩阵连接到hidden层。

这样一来，我们就需要构建两个小矩阵，并进行前后两次矩阵运算 O(V, E) + O(E, H)

**跨层参数共享：**

原始BERT中，每一层Transformer的参数是不共享的，而在ALBERT中所有Transformer Layer的参数都是一样的

**句子顺序预测任务：**

在之前提到的，Bert的NSP任务可能导致模型依赖主题进行区分。

ALBERT选择的正样本是连续的两段文本，负样本是把这两段本文调换顺序，最后让模型学习句子顺序是否正确。

这种方式去除了主题识别的影响，使预训练更关注句子的连贯性。



#### Electra

Electra提出了新的预训练任务和框架。把生成式的 带掩码的语言模型 预训练任务改成了判别式的 替换词检测，判断当前token是否被语言模型替换过。

比如输入一个句子，加上mask，经过一个较小的MLM模型对mask去预测，将预测的句子拿到判别器去判断，那些词是原始的，那些词是被替换了的。



### word2vec原理，如何得到词向量

https://www.zhihu.com/question/44832436/answer/131725613

它将每个词映射到一个固定长度的向量，这些向量能更好地表达不同**词之间的相似性和类比关系**。

word2vec 本质上是一种**降维**操作——把词语从 one-hot 形式的表示降维到更低维度的向量的表示。

以CBOW模型为例，CBOW模型根据上下文词来预测中心词

1. 输入层：上下文单词的onehot. {假设单词向量空间dim为V，上下文单词个数为C}
2. 所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W}
3. 所得的向量 {因为是onehot所以为向量} 相加求平均作为[隐层向量](https://www.zhihu.com/search?q=隐层向量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A266068967}), size为1*N.
4. 乘以输出权重矩阵W' {N*V}
5. 得到向量 {1*V} 激活函数处理得到V-dim概率分布 {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）
6. 与true label的onehot做比较，误差越小越好

反复训练，得到**输入层到隐藏层的参数矩阵**，矩阵中每一行的转置即是对应词的词向量





### 如何分词，分词原理

基于隐马模型的维特比算法

最大匹配算法

最少分词法(最短路径法)





### lightGBM是什么模型，GBDT原理

```

```



### 数据集分成几份，每份的作用是什么

数据集的划分一般有三种方法：

1. 按一定比例划分为训练集和测试集

   直接将数据随机划分为训练集和测试集，然后使用训练集来生成模型，再用测试集来测试模型的**正确率**和**误差**

2. 训练集、验证集、测试集法

   首先将数据集划分为训练集和测试集，由于模型的构建过程中也需要检验模型，检验模型的配置，以及训练程度，过拟合还是欠拟合，所以会将训练数据再划分为训练集和验证集。当模型“通过”验证集之后，我们再使用测试集测试模型的最终效果，评估模型的准确率，以及误差等。

3. 交叉验证法

   交叉验证一般采用k折交叉验证，即k-fold cross validation。在这种数据集划分法中，我们将数据集划分为k个子集，每个子集均做一次测试集，每次将其余的作为训练集。在交叉验证时，我们重复训练k次，每次选择一个子集作为测试集，并将k次的平均交叉验证的正确率作为最终的结果。



### 怎么知道模型过拟合了

模型在训练集（原始数据集）上误差非常低，在一个全新的测试集（全新数据集）上，模型的误差又非常大。



### 如何应对过拟合

1. 获取更多数据（源头获取、数据增强）。让模型「看见」尽可能多的「例外情况」，它就会不断修正自己，从而得到更好的结果。
2. 选择合适的模型。
   - 减少网络的层数、神经元个数等均可以限制网络的拟合能力
   - 训练时间 Early stopping。在初始化网络的时候一般都是初始为较小的权值。训练时间越长，部分网络权值可能越大。如果我们在合适时间停止训练，就可以将网络的能力限制在一定范围内。
   - 限制权值 Weight-decay，也叫正则化（regularization）。直接将权值的大小加入到 Cost 里
   - 增加噪声 Noise。在输入中、权值中
3. Dropout（详见下一条）
4. 集成学习



### dropout是什么

在每个训练批次中，在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为**它不会太依赖某些局部的特征**。

一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）。





### 过拟合什么情况下比较容易产生

1）训练样本太少；2）网络参数太大



### 能讲下BERT么

```

```



### BERT和GPT、Elmo比较

https://zhuanlan.zhihu.com/p/72309137

https://www.jianshu.com/p/2daf69f8408f

ELMO的基本思想是利用双向的LSTM结构。forward学习的是根据前面的词的信息预测当前词的概率，backward学习的根据后面的词预测当前词的概率。bidirectional LSTM就是将两者结合起来，使两个条件概率最大化。

GPT-2用的是transformer中去掉中间Encoder-Decoder Attention层的decoder。最大化语句序列出现的概率，不过这里的语言模型仅仅是forward单向的，而不是双向的。

Bert使用的是transformer的encoder。



### 介绍文本表示方法

```

```



### L1正则化为什么稀疏

1）从梯度角度

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gz1x7lo6ghj30tm0dkgm8.jpg" alt="截屏2022-02-04 23.58.53" style="zoom: 40%;" /> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gz1x7wr9n5j30t20e4aam.jpg" alt="截屏2022-02-04 23.59.11" style="zoom:40%;" /> 

引入L2正则时，代价函数在0处的导数仍是d0，无变化。而引入L1正则后，代价函数在0处的导数有一个突变。从d0+λ到d0−λ，若d0+λ和d0−λ异号，则在0处会是一个极小值点。因此，优化时，很可能优化到该极小值点上，即w=0处。通常越大的λ 可以让代价函数在参数为0时取到最小值。



### LDA

```

```







## 编程

### 实现sqrt函数，结果保留5位小数







### 二叉树路径上和为target的最长路径



### 合并两个有序链表



### 求一个字符串中连续出现次数最多的子串



### 二分查找



### 手写k-means



### 删除链表中的重复节点





## 场景题

现在有一些新闻，包含军事、体育、经济等，想分出它属于哪个类，该怎么做



以过去一年的所有对话为数据，构建一个对话系统



给出淘宝的总商品总量，估算拼多多的商品总量，分类后分别抽样，每个类前面乘一个权重，权重是单价分之一。